{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN1G1h4GzDIA"
      },
      "source": [
        "### filoTips Instructions:\n",
        "\n",
        "1.   Collect images of your cells making filopodia. Please provide one channel: fluorescent tip-enriched protein that robustly labels cell body and filopodia tips. Z-stack max projection recommended.\n",
        "1.   Drag your **images** into \"**Files**\" on the left side of the notebook\n",
        "1.   Input the **micron/pixel** ratio\n",
        "1.   Select \"Default\" or \"Custom\" DL model type\n",
        "2.   If Custom model is selected, please provide the Google Drive link to a compressed .zip folder containing your model. This can be found by right clicking your .zip model > Share > Share > General access > Anyone with link > Copy Link > Paste link here:\n",
        "1.  Do you want to download the model predictions? If so check \"**download_predictions**\"\n",
        "1.   Check \"**filoSpace**\" if you want inter-filopodial spacing information (Note: high image resolution recommended, experimental)\n",
        "1.   Check \"**comparative_analysis**\" if you want a limited statistical summary (ex. t-test, one-way anova, violin plots)? Must have 2-4 variables or experimental conditions to compare and each image file name must contain a unique string to assign to a specific group. That unique string must be provided for each variable/condition.\n",
        "1.   Figure DPI: Please provide figure DPI. This will impact filoTips annotation resolution and can be adjusted accordingly. Recommended start value is \"300\".\n",
        "\n",
        "2.   Click \"**Runtime**\" then \"**Run all**\"\n",
        "\n",
        "Note: If Step 4 fails, check Step 3 for a warning that too many users have downloaded the file/model recently. If so, 1- click the drive link found in the error msg to download the model, 2- drag and drop the model in \"Files\" where the original images were uploaded, and 3- click Runtime > Run After.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYJWgPOZvna3",
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "#@title 1) Input the pixel/micron ratio and load requirements from ZeroCostDL4Mic (requirements from \"U-Net_2D_Multilabel\" notebook)\n",
        "from __future__ import print_function\n",
        "\n",
        "#code from ZeroCostDL4Mic 1.1\n",
        "!pip install data\n",
        "!pip install fpdf\n",
        "!pip install h5py==2.10\n",
        "!pip install imagecodecs\n",
        "\n",
        "#code from ZeroCostDL4Mic 1.3\n",
        "Notebook_version = '1.13'\n",
        "Network = 'U-Net (2D) multilabel'\n",
        "\n",
        "import imagecodecs\n",
        "from builtins import any as b_any\n",
        "\n",
        "def get_requirements_path():\n",
        "    # Store requirements file in 'contents' directory\n",
        "    current_dir = os.getcwd()\n",
        "    dir_count = current_dir.count('/') - 1\n",
        "    path = '../' * (dir_count) + 'requirements.txt'\n",
        "    return path\n",
        "\n",
        "def filter_files(file_list, filter_list):\n",
        "    filtered_list = []\n",
        "    for fname in file_list:\n",
        "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
        "            filtered_list.append(fname)\n",
        "    return filtered_list\n",
        "\n",
        "def build_requirements_file(before, after):\n",
        "    path = get_requirements_path()\n",
        "\n",
        "    # Exporting requirements.txt for local run\n",
        "    !pip freeze > $path\n",
        "\n",
        "    # Get minimum requirements file\n",
        "    #df = pd.read_csv(path, delimiter = \"\\n\")\n",
        "    df = pd.read_csv(path)\n",
        "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
        "    req_list_temp = df.values.tolist()\n",
        "    req_list = [x[0] for x in req_list_temp]\n",
        "\n",
        "    # Replace with package name and handle cases where import name is different to module name\n",
        "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
        "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
        "    filtered_list = filter_files(req_list, mod_replace_list)\n",
        "\n",
        "    file=open(path,'w')\n",
        "    for item in filtered_list:\n",
        "        file.writelines(item + '\\n')\n",
        "\n",
        "    file.close()\n",
        "\n",
        "import sys\n",
        "before = [str(m) for m in sys.modules]\n",
        "\n",
        "#@markdown\n",
        "output_filename = \"filotips\" #@param {type:\"string\"}\n",
        "um_per_pixel =  0.1099#@param {type:\"number\"}\n",
        "model_type = \"Custom\" #@param [\"Default\", \"Custom\"]\n",
        "custom_model_DriveLink= 'https://drive.google.com/file/d/17wetJfU-oqwZEmZ1c63uRHHUECs3e3Oa/view?usp=drive_link' #@param {type:\"string\"}\n",
        "temp = custom_model_DriveLink.replace('https://drive.google.com/file/d/','')\n",
        "temp = temp.replace('/view?usp=drive_link','')\n",
        "temp = temp.replace('/view?usp=sharing','')\n",
        "custom_model_DriveLink = temp\n",
        "download_predictions = False #@param {type:\"boolean\"}\n",
        "filoSpace = False #@param {type:\"boolean\"}\n",
        "comparative_analysis = False #@param {type:\"boolean\"}\n",
        "Condition_1 = \"\" #@param {type:\"string\"}\n",
        "Condition_2 = \"\" #@param {type:\"string\"}\n",
        "Condition_3 = \"\" #@param {type:\"string\"}\n",
        "Condition_4 = \"\" #@param {type:\"string\"}\n",
        "Annotation_DPI = 300 #@param {type:'number'}\n",
        "pixel_micron=1/um_per_pixel\n",
        "\n",
        "#As this notebokk depends mostly on keras which runs a tensorflow backend (which in turn is pre-installed in colab)\n",
        "#only the data library needs to be additionally installed.\n",
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "# print(tensorflow.__version__)\n",
        "# print(\"Tensorflow enabled.\")\n",
        "\n",
        "\n",
        "# Keras imports\n",
        "from keras import models\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger # we currently don't use any other callbacks from ModelCheckpoints\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "#from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from keras import backend as keras\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "# General import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from skimage import img_as_ubyte, io, transform\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import sys\n",
        "from math import ceil\n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from pip._internal.operations.freeze import freeze\n",
        "import subprocess\n",
        "# Imports for QC\n",
        "from PIL import Image\n",
        "from scipy import signal\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skimage.util import img_as_uint\n",
        "from skimage.metrics import structural_similarity\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# For sliders and dropdown menu and progress bar\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.feature_extraction import image\n",
        "from skimage import img_as_ubyte, io, transform\n",
        "from skimage.util.shape import view_as_windows\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Suppressing some warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_patches(Training_source, Training_target, patch_width, patch_height, min_fraction):\n",
        "  \"\"\"\n",
        "  Function creates patches from the Training_source and Training_target images.\n",
        "  The steps parameter indicates the offset between patches and, if integer, is the same in x and y.\n",
        "  Saves all created patches in two new directories in the /content folder.\n",
        "\n",
        "  Returns: - Two paths to where the patches are now saved\n",
        "  \"\"\"\n",
        "  DEBUG = False\n",
        "\n",
        "  Patch_source = os.path.join('/content','img_patches')\n",
        "  Patch_target = os.path.join('/content','mask_patches')\n",
        "  Patch_rejected = os.path.join('/content','rejected')\n",
        "\n",
        "  #Here we save the patches, in the /content directory as they will not usually be needed after training\n",
        "  if os.path.exists(Patch_source):\n",
        "    shutil.rmtree(Patch_source)\n",
        "  if os.path.exists(Patch_target):\n",
        "    shutil.rmtree(Patch_target)\n",
        "  if os.path.exists(Patch_rejected):\n",
        "    shutil.rmtree(Patch_rejected)\n",
        "\n",
        "  os.mkdir(Patch_source)\n",
        "  os.mkdir(Patch_target)\n",
        "  os.mkdir(Patch_rejected) #This directory will contain the images that have too little signal.\n",
        "\n",
        "  patch_num = 0\n",
        "\n",
        "  for file in tqdm(os.listdir(Training_source)):\n",
        "\n",
        "    img = io.imread(os.path.join(Training_source, file))\n",
        "    mask = io.imread(os.path.join(Training_target, file),as_gray=True)\n",
        "\n",
        "    if DEBUG:\n",
        "      print(file)\n",
        "      print(img.dtype)\n",
        "\n",
        "    # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n",
        "    patches_img = view_as_windows(img, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n",
        "\n",
        "    patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width,patch_height)\n",
        "    patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width,patch_height)\n",
        "\n",
        "    if DEBUG:\n",
        "      print(all_patches_img.shape)\n",
        "      print(all_patches_img.dtype)\n",
        "\n",
        "    for i in range(patches_img.shape[0]):\n",
        "      img_save_path = os.path.join(Patch_source,'patch_'+str(patch_num)+'.tif')\n",
        "      mask_save_path = os.path.join(Patch_target,'patch_'+str(patch_num)+'.tif')\n",
        "      patch_num += 1\n",
        "\n",
        "      # if the mask conatins at least 2% of its total number pixels as mask, then go ahead and save the images\n",
        "      pixel_threshold_array = sorted(patches_mask[i].flatten())\n",
        "      if pixel_threshold_array[int(round((len(pixel_threshold_array)-1)*(1-min_fraction)))]>0:\n",
        "        io.imsave(img_save_path, img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
        "        io.imsave(mask_save_path, patches_mask[i])\n",
        "      else:\n",
        "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_image.tif', img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
        "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_mask.tif', patches_mask[i])\n",
        "\n",
        "  return Patch_source, Patch_target\n",
        "\n",
        "\n",
        "def estimatePatchSize(data_path, max_width = 512, max_height = 512):\n",
        "\n",
        "  files = os.listdir(data_path)\n",
        "\n",
        "  # Get the size of the first image found in the folder and initialise the variables to that\n",
        "  n = 0\n",
        "  while os.path.isdir(os.path.join(data_path, files[n])):\n",
        "    n += 1\n",
        "  (height_min, width_min) = Image.open(os.path.join(data_path, files[n])).size\n",
        "\n",
        "  # Screen the size of all dataset to find the minimum image size\n",
        "  for file in files:\n",
        "    if not os.path.isdir(os.path.join(data_path, file)):\n",
        "      (height, width) = Image.open(os.path.join(data_path, file)).size\n",
        "      if width < width_min:\n",
        "        width_min = width\n",
        "      if height < height_min:\n",
        "        height_min = height\n",
        "\n",
        "  # Find the power of patches that will fit within the smallest dataset\n",
        "  width_min, height_min = (fittingPowerOfTwo(width_min), fittingPowerOfTwo(height_min))\n",
        "\n",
        "  # Clip values at maximum permissible values\n",
        "  if width_min > max_width:\n",
        "    width_min = max_width\n",
        "\n",
        "  if height_min > max_height:\n",
        "    height_min = max_height\n",
        "\n",
        "  return (width_min, height_min)\n",
        "\n",
        "def fittingPowerOfTwo(number):\n",
        "  n = 0\n",
        "  while 2**n <= number:\n",
        "    n += 1\n",
        "  return 2**(n-1)\n",
        "\n",
        "## TODO: create weighted CE for semantic labels\n",
        "def getClassWeights(Training_target_path):\n",
        "\n",
        "  Mask_dir_list = os.listdir(Training_target_path)\n",
        "  number_of_dataset = len(Mask_dir_list)\n",
        "\n",
        "  class_count = np.zeros(2, dtype=int)\n",
        "  for i in tqdm(range(number_of_dataset)):\n",
        "    mask = io.imread(os.path.join(Training_target_path, Mask_dir_list[i]))\n",
        "    mask = normalizeMinMax(mask)\n",
        "    class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "    class_count[1] += mask.sum()\n",
        "\n",
        "  n_samples = class_count.sum()\n",
        "  n_classes = 2\n",
        "\n",
        "  class_weights = n_samples / (n_classes * class_count)\n",
        "  return class_weights\n",
        "\n",
        "def weighted_binary_crossentropy(class_weights):\n",
        "\n",
        "    def _weighted_binary_crossentropy(y_true, y_pred):\n",
        "        binary_crossentropy = keras.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * class_weights[1] + (1. - y_true) * class_weights[0]\n",
        "        weighted_binary_crossentropy = weight_vector * binary_crossentropy\n",
        "\n",
        "        return keras.mean(weighted_binary_crossentropy)\n",
        "\n",
        "    return _weighted_binary_crossentropy\n",
        "\n",
        "\n",
        "def save_augment(datagen,orig_img,dir_augmented_data=\"/content/augment\"):\n",
        "  \"\"\"\n",
        "  Saves a subset of the augmented data for visualisation, by default in /content.\n",
        "\n",
        "  This is adapted from: https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    os.mkdir(dir_augmented_data)\n",
        "  except:\n",
        "        ## if the preview folder exists, then remove\n",
        "        ## the contents (pictures) in the folder\n",
        "    for item in os.listdir(dir_augmented_data):\n",
        "      os.remove(dir_augmented_data + \"/\" + item)\n",
        "\n",
        "    ## convert the original image to array\n",
        "  x = img_to_array(orig_img)\n",
        "    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n",
        "    #print(x.shape)\n",
        "  x = x.reshape((1,) + x.shape)\n",
        "    #print(x.shape)\n",
        "    ## -------------------------- ##\n",
        "    ## randomly generate pictures\n",
        "    ## -------------------------- ##\n",
        "  i = 0\n",
        "    #We will just save 5 images,\n",
        "    #but this can be changed, but note the visualisation in 3. currently uses 5.\n",
        "  Nplot = 5\n",
        "  for batch in datagen.flow(x,batch_size=1,\n",
        "                            save_to_dir=dir_augmented_data,\n",
        "                            save_format='tif',\n",
        "                            seed=42):\n",
        "    i += 1\n",
        "    if i > Nplot - 1:\n",
        "      break\n",
        "\n",
        "# Generators\n",
        "def buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, subset, batch_size, target_size, validatio_split):\n",
        "  '''\n",
        "  Can generate image and mask at the same time use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
        "\n",
        "  datagen: ImageDataGenerator\n",
        "  subset: can take either 'training' or 'validation'\n",
        "  '''\n",
        "\n",
        "  # Build the dict for the ImageDataGenerator\n",
        "  # non_aug_args = dict(width_shift_range = 0,\n",
        "  #                     height_shift_range = 0,\n",
        "  #                     rotation_range = 0, #90\n",
        "  #                     zoom_range = 0,\n",
        "  #                     shear_range = 0,\n",
        "  #                     horizontal_flip = False,\n",
        "  #                     vertical_flip = False,\n",
        "  #                     fill_mode = 'reflect')\n",
        "  # default params of data generator is without augmentation\n",
        "  mask_load_gen = ImageDataGenerator(dtype='uint8', validation_split=validatio_split)\n",
        "  image_load_gen = ImageDataGenerator(dtype='float32', validation_split=validatio_split, preprocessing_function = normalizePercentile)\n",
        "\n",
        "  image_generator = image_load_gen.flow_from_directory(\n",
        "        os.path.dirname(image_folder_path),\n",
        "        classes = [os.path.basename(image_folder_path)],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        subset = subset,\n",
        "        interpolation = \"bicubic\",\n",
        "        seed = 1)\n",
        "  mask_generator = mask_load_gen.flow_from_directory(\n",
        "        os.path.dirname(mask_folder_path),\n",
        "        classes = [os.path.basename(mask_folder_path)],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        subset = subset,\n",
        "        interpolation = \"nearest\",\n",
        "        seed = 1)\n",
        "\n",
        "  this_generator = zip(image_generator, mask_generator)\n",
        "  for (img,mask) in this_generator:\n",
        "      if subset == 'training':\n",
        "          # Apply the data augmentation\n",
        "          # the same seed should provide always the same transformation and image loading\n",
        "          seed = np.random.randint(100000)\n",
        "          for batch_im in image_datagen.flow(img,batch_size=batch_size, seed=seed):\n",
        "              break\n",
        "          mask = mask.astype(np.float32)\n",
        "          labels = np.unique(mask)\n",
        "          if len(labels)>1:\n",
        "              batch_mask = np.zeros_like(mask, dtype='float32')\n",
        "              for l in range(0, len(labels)):\n",
        "                  aux = (mask==l).astype(np.float32)\n",
        "                  for batch_aux in mask_datagen.flow(aux,batch_size=batch_size, seed=seed):\n",
        "                      break\n",
        "                  batch_mask += l*(batch_aux>0).astype(np.float32)\n",
        "              index = np.where(batch_mask>l)\n",
        "              batch_mask[index]=l\n",
        "          else:\n",
        "              batch_mask = mask\n",
        "\n",
        "          yield (batch_im,batch_mask)\n",
        "\n",
        "      else:\n",
        "          yield (img,mask)\n",
        "\n",
        "\n",
        "def prepareGenerators(image_folder_path, mask_folder_path, datagen_parameters, batch_size = 4, target_size = (512, 512), validatio_split = 0.1):\n",
        "  image_datagen = ImageDataGenerator(**datagen_parameters, preprocessing_function = normalizePercentile)\n",
        "  mask_datagen = ImageDataGenerator(**datagen_parameters)\n",
        "\n",
        "  train_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'training', batch_size, target_size, validatio_split)\n",
        "  validation_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'validation', batch_size, target_size, validatio_split)\n",
        "\n",
        "  return (train_datagen, validation_datagen)\n",
        "\n",
        "\n",
        "# Normalization functions from Martin Weigert\n",
        "def normalizePercentile(x, pmin=1, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    \"\"\"Percentile-based image normalization.\"\"\"\n",
        "\n",
        "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
        "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
        "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
        "\n",
        "\n",
        "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    if dtype is not None:\n",
        "        x   = x.astype(dtype,copy=False)\n",
        "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
        "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
        "        eps = dtype(eps)\n",
        "\n",
        "    try:\n",
        "        import numexpr\n",
        "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
        "    except ImportError:\n",
        "        x =                   (x - mi) / ( ma - mi + eps )\n",
        "\n",
        "    if clip:\n",
        "        x = np.clip(x,0,1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "# Simple normalization to min/max fir the Mask\n",
        "def normalizeMinMax(x, dtype=np.float32):\n",
        "  x = x.astype(dtype,copy=False)\n",
        "  x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
        "  return x\n",
        "\n",
        "\n",
        "# This is code outlines the architecture of U-net. The choice of pooling steps decides the depth of the network.\n",
        "def unet(pretrained_weights = None, input_size = (256,256,1), pooling_steps = 4, learning_rate = 1e-4, verbose=True, labels=2):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    # Downsampling steps\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "\n",
        "    if pooling_steps > 1:\n",
        "      pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "\n",
        "      if pooling_steps > 2:\n",
        "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "        drop4 = Dropout(0.5)(conv4)\n",
        "\n",
        "        if pooling_steps > 3:\n",
        "          pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "          drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "          #Upsampling steps\n",
        "          up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "          merge6 = concatenate([drop4,up6], axis = 3)\n",
        "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    if pooling_steps > 2:\n",
        "      up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n",
        "      if pooling_steps > 3:\n",
        "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "      merge7 = concatenate([conv3,up7], axis = 3)\n",
        "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    if pooling_steps > 1:\n",
        "      up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv3))\n",
        "      if pooling_steps > 2:\n",
        "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "      merge8 = concatenate([conv2,up8], axis = 3)\n",
        "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    if pooling_steps == 1:\n",
        "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "    else:\n",
        "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8)) #activation = 'relu'\n",
        "\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9) #activation = 'relu'\n",
        "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
        "    conv9 = Conv2D(labels, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
        "    conv10 = Conv2D(labels, 1, activation = 'softmax')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = learning_rate), loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "    if verbose:\n",
        "      model.summary()\n",
        "\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights);\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom callback showing sample prediction\n",
        "class SampleImageCallback(Callback):\n",
        "\n",
        "    def __init__(self, model, sample_data, model_path, save=False):\n",
        "        self.model = model\n",
        "        self.sample_data = sample_data\n",
        "        self.model_path = model_path\n",
        "        self.save = save\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      if np.mod(epoch,5) == 0:\n",
        "            sample_predict = self.model.predict_on_batch(self.sample_data)\n",
        "\n",
        "            f=plt.figure(figsize=(16,8))\n",
        "            plt.subplot(1,labels+1,1)\n",
        "            plt.imshow(self.sample_data[0,:,:,0], cmap='gray')\n",
        "            plt.title('Sample source')\n",
        "            plt.axis('off');\n",
        "            for i in range(1, labels):\n",
        "              plt.subplot(1,labels+1,i+1)\n",
        "              plt.imshow(sample_predict[0,:,:,i], interpolation='nearest', cmap='magma')\n",
        "              plt.title('Predicted label {}'.format(i))\n",
        "              plt.axis('off');\n",
        "\n",
        "            plt.subplot(1,labels+1,labels+1)\n",
        "            plt.imshow(np.squeeze(np.argmax(sample_predict[0], axis=-1)), interpolation='nearest')\n",
        "            plt.title('Semantic segmentation')\n",
        "            plt.axis('off');\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            if self.save:\n",
        "                plt.savefig(self.model_path + '/epoch_' + str(epoch+1) + '.png')\n",
        "                random_choice = random.choice(os.listdir(Patch_source))\n",
        "\n",
        "def predict_as_tiles(Image_path, model):\n",
        "\n",
        "  # Read the data in and normalize\n",
        "  Image_raw = io.imread(Image_path, as_gray = True)\n",
        "  Image_raw = normalizePercentile(Image_raw)\n",
        "\n",
        "  # Get the patch size from the input layer of the model\n",
        "  patch_size = model.layers[0].output_shape[0][1:3]\n",
        "  #patch_size = model.layers[0].output_shape[1:3]\n",
        "\n",
        "  # Pad the image with zeros if any of its dimensions is smaller than the patch size\n",
        "  if Image_raw.shape[0] < patch_size[0] or Image_raw.shape[1] < patch_size[1]:\n",
        "    Image = np.zeros((max(Image_raw.shape[0], patch_size[0]), max(Image_raw.shape[1], patch_size[1])))\n",
        "    Image[0:Image_raw.shape[0], 0: Image_raw.shape[1]] = Image_raw\n",
        "  else:\n",
        "    Image = Image_raw\n",
        "\n",
        "  # Calculate the number of patches in each dimension\n",
        "  n_patch_in_width = ceil(Image.shape[0]/patch_size[0])\n",
        "  n_patch_in_height = ceil(Image.shape[1]/patch_size[1])\n",
        "\n",
        "  prediction = np.zeros(Image.shape, dtype = 'uint8')\n",
        "\n",
        "  for x in range(n_patch_in_width):\n",
        "    for y in range(n_patch_in_height):\n",
        "      xi = patch_size[0]*x\n",
        "      yi = patch_size[1]*y\n",
        "\n",
        "      # If the patch exceeds the edge of the image shift it back\n",
        "      if xi+patch_size[0] >= Image.shape[0]:\n",
        "        xi = Image.shape[0]-patch_size[0]\n",
        "\n",
        "      if yi+patch_size[1] >= Image.shape[1]:\n",
        "        yi = Image.shape[1]-patch_size[1]\n",
        "\n",
        "      # Extract and reshape the patch\n",
        "      patch = Image[xi:xi+patch_size[0], yi:yi+patch_size[1]]\n",
        "      patch = np.reshape(patch,patch.shape+(1,))\n",
        "      patch = np.reshape(patch,(1,)+patch.shape)\n",
        "\n",
        "      # Get the prediction from the patch and paste it in the prediction in the right place\n",
        "      predicted_patch = model.predict(patch, batch_size = 1)\n",
        "      prediction[xi:xi+patch_size[0], yi:yi+patch_size[1]] = (np.argmax(np.squeeze(predicted_patch), axis = -1)).astype(np.uint8)\n",
        "\n",
        "\n",
        "  return prediction[0:Image_raw.shape[0], 0: Image_raw.shape[1]]\n",
        "\n",
        "\n",
        "def saveResult(save_path, nparray, source_dir_list, prefix=''):\n",
        "  for (filename, image) in zip(source_dir_list, nparray):\n",
        "      io.imsave(os.path.join(save_path, prefix+os.path.splitext(filename)[0]+'.tif'), image) # saving as unsigned 8-bit image\n",
        "\n",
        "\n",
        "def convert2Mask(image, threshold):\n",
        "  mask = img_as_ubyte(image, force_copy=True)\n",
        "  mask[mask > threshold] = 255\n",
        "  mask[mask <= threshold] = 0\n",
        "  return mask\n",
        "\n",
        "# -------------- Other definitions -----------\n",
        "W  = '\\033[0m'  # white (normal)\n",
        "R  = '\\033[31m' # red\n",
        "prediction_prefix = 'Predicted_'\n",
        "\n",
        "\n",
        "print('-------------------')\n",
        "print('U-Net and dependencies installed.')\n",
        "\n",
        "# Colors for the warning messages\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "\n",
        "#All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
        "#print('Notebook version: '+Notebook_version)\n",
        "#Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
        "#print('Latest notebook version: '+Latest_Notebook_version)\n",
        "#if Notebook_version == Latest_Notebook_version:\n",
        "#  print(\"This notebook is up-to-date.\")\n",
        "#else:\n",
        "#  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
        "\n",
        "\n",
        "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "  # add another cell\n",
        "  if trained:\n",
        "    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
        "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  Header_2 = 'Information for your materials and method:'\n",
        "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "  #print(all_packages)\n",
        "\n",
        "  #Main Packages\n",
        "  main_packages = ''\n",
        "  version_numbers = []\n",
        "  for name in ['tensorflow','numpy','Keras']:\n",
        "    find_name=all_packages.find(name)\n",
        "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
        "    #Version numbers only here:\n",
        "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
        "\n",
        "  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
        "  cuda_version = cuda_version.stdout.decode('utf-8')\n",
        "  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
        "  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
        "  gpu_name = gpu_name.stdout.decode('utf-8')\n",
        "  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
        "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
        "  #print(gpu_name)\n",
        "  loss = str(model.loss)[str(model.loss).find('function')+len('function'):str(model.loss).find('.<')]\n",
        "  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
        "  dataset_size = len(os.listdir(Training_source))\n",
        "\n",
        "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+' and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  if pretrained_model:\n",
        "    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+'  and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  pdf.multi_cell(180, 5, txt = text, align='L')\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(28, 5, txt='Augmentation: ', ln=1)\n",
        "  pdf.set_font('')\n",
        "  if augmentation:\n",
        "    aug_text = 'The dataset was augmented by'\n",
        "    if rotation_range != 0:\n",
        "      aug_text = aug_text+'\\n- rotation'\n",
        "    if horizontal_flip == True or vertical_flip == True:\n",
        "      aug_text = aug_text+'\\n- flipping'\n",
        "    if zoom_range != 0:\n",
        "      aug_text = aug_text+'\\n- random zoom magnification'\n",
        "    if horizontal_shift != 0 or vertical_shift != 0:\n",
        "      aug_text = aug_text+'\\n- shifting'\n",
        "    if shear_range != 0:\n",
        "      aug_text = aug_text+'\\n- image shearing'\n",
        "  else:\n",
        "    aug_text = 'No augmentation was used for training.'\n",
        "  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  if Use_Default_Advanced_Parameters:\n",
        "    pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
        "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
        "  pdf.ln(1)\n",
        "  html = \"\"\"\n",
        "  <table width=40% style=\"margin-left:0px;\">\n",
        "    <tr>\n",
        "      <th width = 50% align=\"left\">Parameter</th>\n",
        "      <th width = 50% align=\"left\">Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>number_of_epochs</td>\n",
        "      <td width = 50%>{0}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>patch_size</td>\n",
        "      <td width = 50%>{1}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>batch_size</td>\n",
        "      <td width = 50%>{2}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>number_of_steps</td>\n",
        "      <td width = 50%>{3}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>percentage_validation</td>\n",
        "      <td width = 50%>{4}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>initial_learning_rate</td>\n",
        "      <td width = 50%>{5}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>pooling_steps</td>\n",
        "      <td width = 50%>{6}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>min_fraction</td>\n",
        "      <td width = 50%>{7}</td>\n",
        "  </table>\n",
        "  \"\"\".format(number_of_epochs, str(patch_width)+'x'+str(patch_height), batch_size, number_of_steps, percentage_validation, initial_learning_rate, pooling_steps, min_fraction)\n",
        "  pdf.write_html(html)\n",
        "\n",
        "  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(28, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
        "  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread('/content/TrainingDataExample_Unet2D.png').shape\n",
        "  pdf.image('/content/TrainingDataExample_Unet2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "  # if Use_Data_augmentation:\n",
        "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
        "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
        "  pdf.ln(3)\n",
        "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "  pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n",
        "\n",
        "  print('------------------------------')\n",
        "  print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
        "\n",
        "def qc_pdf_export():\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  Network = 'Unet 2D'\n",
        "\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(2)\n",
        "  pdf.cell(190, 5, txt = 'Loss curves', ln=1, align='L')\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
        "  if os.path.exists(full_QC_model_path+'/Quality Control/lossCurvePlots.png'):\n",
        "    pdf.image(full_QC_model_path+'/Quality Control/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/12), h = round(exp_size[0]/3))\n",
        "  else:\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size=10)\n",
        "    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.',align='L')\n",
        "  pdf.ln(2)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(3)\n",
        "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
        "  pdf.image(full_QC_model_path+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  html = \"\"\"\n",
        "  <body>\n",
        "  <font size=\"10\" face=\"Courier New\" >\n",
        "  <table width=60% style=\"margin-left:0px;\">\"\"\"\n",
        "  with open(full_QC_model_path+'/Quality Control/QC_metrics_'+QC_model_name+'.csv', 'r') as csvfile:\n",
        "    metrics = csv.reader(csvfile)\n",
        "    header = next(metrics)\n",
        "    image = header[0]\n",
        "    IoU = header[-1]\n",
        "    header = \"\"\"\n",
        "    <tr>\n",
        "    <th width = 33% align=\"center\">{0}</th>\n",
        "    <th width = 33% align=\"center\">{1}</th>\n",
        "    </tr>\"\"\".format(image,IoU)\n",
        "    html = html+header\n",
        "    i=0\n",
        "    for row in metrics:\n",
        "      i+=1\n",
        "      image = row[0]\n",
        "      IoU = row[-1]\n",
        "      cells = \"\"\"\n",
        "        <tr>\n",
        "          <td width = 33% align=\"center\">{0}</td>\n",
        "          <td width = 33% align=\"center\">{1}</td>\n",
        "        </tr>\"\"\".format(image,str(round(float(IoU),3)))\n",
        "      html = html+cells\n",
        "    html = html+\"\"\"</body></table>\"\"\"\n",
        "\n",
        "  pdf.write_html(html)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "\n",
        "  pdf.ln(3)\n",
        "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
        "\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "  pdf.output(full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "  print('------------------------------')\n",
        "  print('QC PDF report exported as '+full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "# Build requirements file for local run\n",
        "after = [str(m) for m in sys.modules]\n",
        "build_requirements_file(before, after)\n",
        "\n",
        "#code from ZeroCostDL4Mic 2.1\n",
        "\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.')\n",
        "  print('Did you change your runtime ?')\n",
        "  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "  !nvidia-smi\n",
        "\n",
        "# from tensorflow.python.client import device_lib\n",
        "# device_lib.list_local_devices()\n",
        "\n",
        "# print the tensorflow version\n",
        "print('Tensorflow version is ' + str(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ooDOCulppJ",
        "cellView": "form"
      },
      "source": [
        "#@title 2) Load requirements for filoTips\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile as tiff\n",
        "import os\n",
        "import seaborn as sns\n",
        "import statistics as stats\n",
        "import math\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "!pip install researchpy\n",
        "import researchpy as rp\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "def get_distance(x2,x1,y2,y1):\n",
        "    return math.sqrt((x2-x1)**2+(y2-y1)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqxx2jWoKXZ",
        "cellView": "form"
      },
      "source": [
        "#@title 3) Download the deep learning agent and prepare for predictions\n",
        "os.chdir('/content')\n",
        "if not os.path.exists('filoTips predictions'):\n",
        "  os.makedirs('filoTips predictions')\n",
        "if not os.path.exists('filoTips source'):\n",
        "  os.makedirs('filoTips source')\n",
        "\n",
        "files=sorted(glob.glob('*.tif'))\n",
        "if len(files)>0:\n",
        "  for file in files:\n",
        "    temp=file.replace('.tif','.tiff')\n",
        "    os.replace(file,temp)\n",
        "\n",
        "files=sorted(glob.glob('*.TIF'))\n",
        "if len(files)>0:\n",
        "  for file in files:\n",
        "    temp=file.replace('.TIF','.tiff')\n",
        "    os.replace(file,temp)\n",
        "\n",
        "files=sorted(glob.glob('*.tiff'))\n",
        "for file in files:\n",
        "  os.replace(file,'filoTips source/'+file)\n",
        "if model_type=='Default':\n",
        "  !gdown --id 1zh5j_VL380ebyQxDCJ5N2fhOxWqkFQsC\n",
        "  !unzip -u \"StanleyV3.zip\" -d \"Stanley\"\n",
        "if model_type=='Custom':\n",
        "  !gdown --id $custom_model_DriveLink\n",
        "  zipp=glob.glob('*.zip')[0]\n",
        "  custom_model_name = zipp.replace('.zip','')\n",
        "  zipp = '\"'+zipp+'\"'\n",
        "  !unzip $zipp\n",
        "\n",
        "os.chdir('filoTips source')\n",
        "for file in files:\n",
        "  img=cv2.imread(file,-1)\n",
        "  img=cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "  tiff.imsave(file,img)\n",
        "\n",
        "os.chdir('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2TD5p7MZrEb",
        "cellView": "form"
      },
      "source": [
        "#@title 4) Have the model generate masks of cell bodies, filopodia, and background (includes code from \"U-Net_2D_Multilabel\" ZeroCostDL4Mic notebook)\n",
        "\n",
        "\n",
        "# ------------- Initial user input ------------\n",
        "#@markdown\n",
        "Data_folder = '/content/filoTips source'\n",
        "Results_folder = '/content/filoTips predictions'\n",
        "if model_type=='Default':\n",
        "  Prediction_model_folder = \"/content/Stanley/Stanley\"\n",
        "if model_type=='Custom':\n",
        "  Prediction_model_folder = '/content/'+custom_model_name+'/'+custom_model_name\n",
        "  if not os.path.exists(Prediction_model_folder):\n",
        "    Prediction_model_folder = '/content/'+custom_model_name\n",
        "\n",
        "\n",
        "#prediction_model_folder = '/content/'+custom_model_name\n",
        "Use_the_current_trained_model = False\n",
        "\n",
        "#Here we find the loaded model name and parent path\n",
        "Prediction_model_name = os.path.basename(Prediction_model_folder)\n",
        "Prediction_model_path = os.path.dirname(Prediction_model_folder)\n",
        "\n",
        "\n",
        "# ------------- Failsafes ------------\n",
        "if (Use_the_current_trained_model):\n",
        "  print(\"Using current trained network\")\n",
        "  Prediction_model_name = model_name\n",
        "  Prediction_model_path = model_path\n",
        "\n",
        "full_Prediction_model_path = os.path.join(Prediction_model_path, Prediction_model_name)\n",
        "if os.path.exists(full_Prediction_model_path):\n",
        "  print(\"The \"+Prediction_model_name+\" network will be used.\")\n",
        "else:\n",
        "  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
        "\n",
        "\n",
        "# ------------- Prepare the model and run predictions ------------\n",
        "\n",
        "# Load the model and prepare generator\n",
        "\n",
        "\n",
        "\n",
        "unet = load_model(os.path.join(Prediction_model_path, Prediction_model_name, 'weights_best.hdf5'), custom_objects={'_weighted_binary_crossentropy': weighted_binary_crossentropy(np.ones(2))})\n",
        "#Input_size = unet.layers[0].output_shape[1:3]\n",
        "Input_size= unet.layers[0].output_shape[0][1:3]\n",
        "print('Model input size: '+str(Input_size[0])+'x'+str(Input_size[1]))\n",
        "\n",
        "\n",
        "# Create a list of sources\n",
        "source_dir_list = os.listdir(Data_folder)\n",
        "number_of_dataset = len(source_dir_list)\n",
        "print('Number of dataset found in the folder: '+str(number_of_dataset))\n",
        "\n",
        "predictions = []\n",
        "for i in tqdm(range(number_of_dataset)):\n",
        "  predictions.append(predict_as_tiles(os.path.join(Data_folder, source_dir_list[i]), unet))\n",
        "  #predictions.append(prediction(os.path.join(Data_folder, source_dir_list[i]), os.path.join(Prediction_model_path, Prediction_model_name)))\n",
        "\n",
        "\n",
        "# Save the results in the folder along with the masks according to the set threshold\n",
        "saveResult(Results_folder, predictions, source_dir_list, prefix=prediction_prefix)\n",
        "\n",
        "\n",
        "# ------------- For display ------------\n",
        "print('--------------------------------------------------------------')\n",
        "os.chdir('filoTips predictions')\n",
        "files=sorted(glob.glob('*.tif'))\n",
        "for file in files:\n",
        "  name=file.replace('.tif','.tiff')\n",
        "  os.rename(file,name)\n",
        "os.chdir('/content')\n",
        "\n",
        "def show_prediction_mask(file=os.listdir(Data_folder)):\n",
        "\n",
        "  plt.figure(figsize=(10,6))\n",
        "  # Wide-field\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.axis('off')\n",
        "  img_Source = plt.imread(os.path.join(Data_folder, file))\n",
        "  plt.imshow(img_Source, cmap='gray')\n",
        "  plt.title('Source image',fontsize=15)\n",
        "  # Prediction\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.axis('off')\n",
        "  img_Prediction = plt.imread(os.path.join(Results_folder, prediction_prefix+file))\n",
        "  plt.imshow(img_Prediction, cmap='gray')\n",
        "  plt.title('Prediction',fontsize=15)\n",
        "\n",
        "interact(show_prediction_mask);\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyLXFDqY5yYZ",
        "cellView": "form"
      },
      "source": [
        "#@title 5) Use the masks to get information about cell bodies and filopodia\n",
        "os.chdir('/content/filoTips predictions')\n",
        "temp=sorted(glob.glob('*.tiff'))\n",
        "masks,images=[],[]\n",
        "for mask in temp:\n",
        "  masks.append(mask.replace('Predicted_',''))\n",
        "  images.append(mask.replace('Predicted_',''))\n",
        "  new_mask=(mask.replace('Predicted_',''))\n",
        "  os.rename(mask,new_mask)\n",
        "\n",
        "for q in range(0,len(images)):\n",
        "  os.chdir('/content/filoTips predictions')\n",
        "  mask=cv2.imread(images[q],-1)\n",
        "  os.chdir('/content/filoTips source')\n",
        "  img=cv2.imread(images[q],-1)\n",
        "  img_copy=cv2.imread(images[q])\n",
        "  img_copy[np.where(img_copy>0)]=0\n",
        "  img_copy2=np.copy(img_copy)\n",
        "  img_copy3=np.copy(img_copy)\n",
        "  img_copy4=np.copy(img_copy)\n",
        "  cell=np.copy(mask)\n",
        "  cell[np.where(cell==2)]=0\n",
        "  filo_tip=np.copy(mask)\n",
        "  filo_tip[np.where(filo_tip==1)]=0\n",
        "\n",
        "  contours, hierarchy = cv2.findContours(cell, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  mean_body_vals,mean_cortex_vals,centroids,areas,perimeters,aspect_ratios,leading_edge_vals,leading_body,cortex_body,all_body_x,all_body_y,all_cortex_x,all_cortex_y,cortex_means,body_means,lead_means,centroids,side_rear_means,body_sums=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "  white,yellow,blue,green,pink,red,orange,black,light_orange,gray=(255,255,255),(204,204,0),(51,153,255),(0,204,0),(255,0,127),(255,51,51),(153,76,0),(0,0,0),(255,178,102),(160,160,160)\n",
        "  gray1,gray2,gray3,gray4,purple=(224,224,224),(192,192,192),(160,160,160),(128,128,128),(102,0,51)\n",
        "  for i in range(0,len(contours)):\n",
        "      M=cv2.moments(contours[i])\n",
        "      if M['m00']!=0:\n",
        "          area=cv2.contourArea(contours[i])/(pixel_micron**2)\n",
        "          if area>25:\n",
        "              centroid=(int(M['m10']/M['m00']),int(M['m01']/M['m00']))\n",
        "              centroids.append(centroid)\n",
        "              areas.append(cv2.contourArea(contours[i])/(pixel_micron**2))\n",
        "              perimeters.append(cv2.arcLength(contours[i],True)/(pixel_micron))\n",
        "              rect=cv2.minAreaRect(contours[i])\n",
        "              wh=rect[1]\n",
        "              w=np.min(wh)\n",
        "              h=np.max(wh)\n",
        "              aspect_ratios.append(float(w)/h)\n",
        "              img_copy2=cv2.drawContours(np.copy(img_copy),contours,i,white,-1)\n",
        "              img_copy2=cv2.drawContours(img_copy2,contours,i,black,20)#was15\n",
        "              img_copy2=np.where((img_copy2==list(white)).all(axis=2))\n",
        "              img_copy8=cv2.drawContours(np.copy(img_copy),contours,i,purple,-1)\n",
        "              img_copy8=cv2.drawContours(img_copy8,contours,i,black,9)\n",
        "              img_copy8=np.where((img_copy8==list(purple)).all(axis=2))\n",
        "              img_copy3=cv2.drawContours(np.copy(img_copy3),contours,i,gray1,-1)\n",
        "              img_copy3=cv2.drawContours(img_copy3,contours,i,black,5)\n",
        "              img_copy3[img_copy8]=purple\n",
        "              img_copy3[img_copy2]=gray2\n",
        "\n",
        "              body_vals=img[np.where((img_copy3==list(gray2)).all(axis=2))]\n",
        "              body_means.append(np.mean(body_vals))\n",
        "              body_sums.append(np.sum(body_vals))\n",
        "              cortex_vals=img[np.where((img_copy3==list(gray1)).all(axis=2))]\n",
        "              cortex_means.append(np.mean(cortex_vals))\n",
        "              lead_ind=int(np.argmax(cortex_vals==max(cortex_vals)))\n",
        "              cortex_coords=np.where((img_copy3==list(gray1)).all(axis=2))\n",
        "              y1,x1=np.where((img_copy3==list(gray1)).all(axis=2))[0],np.where((img_copy3==list(gray1)).all(axis=2))[1]\n",
        "              img_copy4=cv2.circle(img_copy4, (x1[lead_ind],y1[lead_ind]), 15, white,thickness=-1)\n",
        "              circle_coords=np.where((img_copy4==list(white)).all(axis=2))\n",
        "              circle=[]\n",
        "              leading_coords=[]\n",
        "              lead_x,lead_y=[],[]\n",
        "              for y in range(0,len(circle_coords[0])):\n",
        "                  circle.append((circle_coords[0][y],circle_coords[1][y]))\n",
        "              for x in range(0,len(cortex_coords[0])):\n",
        "                  c_coords=(cortex_coords[0][x],cortex_coords[1][x])\n",
        "                  if c_coords in circle:\n",
        "                      lead_x.append(c_coords[1])\n",
        "                      lead_y.append(c_coords[0])\n",
        "              lead_coord=lead_y,lead_x\n",
        "              img_copy3[lead_coord]=gray3\n",
        "              lead_vals=img[lead_coord]\n",
        "              lead_means.append(np.mean(lead_vals))\n",
        "              img_copy4=cv2.circle(img_copy4, (x1[lead_ind],y1[lead_ind]), 15, gray1,thickness=-1)\n",
        "              side_rear_means.append(np.mean(img[np.where((img_copy3==list(gray1)).all(axis=2))]))\n",
        "              img_copy3[np.where((img_copy3==list(gray1)).all(axis=2))]=orange\n",
        "              img_copy3[np.where((img_copy3==list(gray2)).all(axis=2))]=yellow\n",
        "              img_copy3[np.where((img_copy3==list(gray3)).all(axis=2))]=blue\n",
        "              img_copy3[np.where((img_copy3==list(purple)).all(axis=2))]=gray4\n",
        "  img_copy5=np.copy(img_copy3)\n",
        "  img_copy5[np.where((img_copy3==list(orange)).all(axis=2))]=blue\n",
        "  filo_tip[np.where((img_copy3==list(yellow)).all(axis=2))]=0\n",
        "\n",
        "\n",
        "\n",
        "  #get filo information\n",
        "  contours, hierarchy = cv2.findContours(filo_tip, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  filo_centroids,filo_lengths_pix,filo_lengths_um,filo_means,temp_filo,cell_assignment,filo_areas,filo_sums=[],[],[],[],[],[],[],[]\n",
        "  for i in range(0,len(contours)):\n",
        "      M=cv2.moments(contours[i])\n",
        "      if M['m00']!=0:\n",
        "          centroid=(int(M['m10']/M['m00']),int(M['m01']/M['m00']))\n",
        "          all_blue=np.where((img_copy5==list(blue)).all(axis=2))\n",
        "          distances=[]\n",
        "          for d in range(0,len(all_blue[0])):\n",
        "              y2,y1,x2,x1=centroid[1],all_blue[0][d],centroid[0],all_blue[1][d]\n",
        "              distance=get_distance(x2, x1, y2, y1)\n",
        "              distances.append(distance)\n",
        "          if len(distances)==0:\n",
        "            break\n",
        "          ind=distances.index(min(distances))\n",
        "          length=min(distances)/pixel_micron\n",
        "\n",
        "          if length >0 and length <10:\n",
        "              filo_centroids.append(centroid)\n",
        "              filo_lengths_pix.append(min(distances))\n",
        "              filo_lengths_um.append(min(distances)/pixel_micron)\n",
        "              area=cv2.contourArea(contours[i])/(pixel_micron**2)\n",
        "              img_copy3=cv2.circle(img_copy3,centroid,radius=1,color=white,thickness=-1,)\n",
        "\n",
        "              y=np.where((img_copy3==list(white)).all(axis=2))[0]\n",
        "              x=np.where((img_copy3==list(white)).all(axis=2))[1]\n",
        "              filo_means.append(np.mean(img[y,x]))\n",
        "              filo_sums.append(np.sum(img[y,x]))\n",
        "              filo_areas.append(area)\n",
        "              distances=[]\n",
        "              for cent in centroids:\n",
        "                  y2,y1,x2,x1=centroid[1],cent[1],centroid[0],cent[0]\n",
        "                  distances.append(get_distance(x2, x1, y2, y1))\n",
        "              cell_assignment.append(distances.index(min(distances))+1)\n",
        "              cell_assignments=distances.index(min(distances))\n",
        "              img_copy3[y,x]=pink\n",
        "              temp_filo.append(img[y,x])\n",
        "              coord=(all_blue[1][ind],all_blue[0][ind])\n",
        "              img_copy5=cv2.circle(img_copy5,coord,1,red)\n",
        "              img_copy5=cv2.line(img_copy5,coord,centroids[cell_assignments],red,2)\n",
        "              img_copy3=cv2.line(img_copy3,centroid,coord,pink,1)\n",
        "\n",
        "\n",
        "  #calculate spacing (filoSpace)\n",
        "  if filoSpace == True:\n",
        "    img_copy6=np.copy(img_copy5)\n",
        "    img_copy7=np.copy(img_copy)\n",
        "    img_copy6[np.where((img_copy5!=list(blue)).all(axis=2))]=black\n",
        "    img_copy6 = cv2.cvtColor(img_copy6, cv2.COLOR_BGR2GRAY)\n",
        "    contours, hierarchy = cv2.findContours(img_copy6, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    cell_ind=[]\n",
        "    space_perimeters=[]\n",
        "    for i in range(0,len(contours)):\n",
        "      M=cv2.moments(contours[i])\n",
        "      if M['m00']!=0:\n",
        "        perimeter=cv2.arcLength(contours[i],False)/(pixel_micron)\n",
        "        centroid=(int(M['m10']/M['m00']),int(M['m01']/M['m00']))\n",
        "        if perimeter<20:\n",
        "          img_copy7=cv2.drawContours(np.copy(img_copy7),contours,i,white,-1)\n",
        "          distances2=[]\n",
        "          for d in range(0,len(centroids)):\n",
        "            x1,y1,x2,y2=centroid[0],centroid[1],centroids[d][0],centroids[d][1]\n",
        "            distance=get_distance(x2, x1, y2, y1)\n",
        "            distances2.append(distance)\n",
        "          cell_ind.append(distances2.index(min(distances2))+1)\n",
        "          space_perimeters.append(perimeter)\n",
        "    cell_ind=np.asarray(cell_ind)\n",
        "    space_perimeters=np.asarray(space_perimeters)\n",
        "    inds=np.asarray(list(set(cell_ind)))\n",
        "    space_cell_num,space_cell_avg=[],[]\n",
        "    for i in inds:\n",
        "      cell_inds=np.where(cell_ind==i)[0]\n",
        "      space_cell_num.append(i)\n",
        "      space_cell_avg.append(np.mean(space_perimeters[cell_inds]))\n",
        "\n",
        "  #plot centroids and cell numbers\n",
        "  k=0\n",
        "  for centroid in centroids:\n",
        "      k+=1\n",
        "      img_copy3=cv2.circle(img_copy3,centroid,5,green,2)\n",
        "      img_copy3=cv2.putText(img_copy3,text=str(k),org=centroid,fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(254,254,254), thickness=2, lineType=cv2.LINE_AA)\n",
        "\n",
        "  #calculate cortex/cell and filo/cell ratios\n",
        "  cortex_body= [i/j for i,j in zip(cortex_means,body_means)]\n",
        "  lead_body=[i/j for i,j in zip(lead_means,body_means)]\n",
        "  side_rear_body=[i/j for i,j in zip(side_rear_means,body_means)]\n",
        "  lead_side_rear=[i/j for i,j in zip(lead_means,side_rear_means)]\n",
        "  final_assignments=[]\n",
        "  for i in range(1,len(centroids)+1):\n",
        "      if i not in cell_assignment:\n",
        "          final_assignments.append(0)\n",
        "      if i in cell_assignment:\n",
        "          final_assignments.append(cell_assignment.count(i))\n",
        "  cell_num,filo_num=list(range(1,len(centroids)+1)),list(range(1,len(filo_centroids)+1))\n",
        "  filo_cell_vals=[]\n",
        "  for cell in cell_assignment:\n",
        "      filo_cell_vals.append(body_means[cell-1])\n",
        "  filo_body=[i/j for i,j in zip(filo_means,filo_cell_vals)]\n",
        "  filoDensity = [i/j*10 for i,j in zip(final_assignments,perimeters)]\n",
        "\n",
        "  #include spacing summary information\n",
        "  if filoSpace==True:\n",
        "    for i in cell_num:\n",
        "      if i not in space_cell_num:\n",
        "        space_cell_num.append(i)\n",
        "        space_cell_avg.append('nan')\n",
        "    zipped_lists = zip(space_cell_num, space_cell_avg)\n",
        "    sorted_pairs = sorted(zipped_lists)\n",
        "    tuples = zip(*sorted_pairs)\n",
        "    space_cell_num, space_cell_avg = [ list(tuple) for tuple in  tuples]\n",
        "\n",
        "  #make output file and pool analysis\n",
        "  os.chdir('/content')\n",
        "  if not os.path.exists('filoTips_Output'):\n",
        "      os.makedirs('filoTips_Output')\n",
        "  exp_num_cell=np.repeat(images[q],len(cell_num))\n",
        "  exp_num_filo=np.repeat(images[q],len(filo_num))\n",
        "  if filoSpace==True:\n",
        "    cell_dict={'Experiment Name':exp_num_cell,'Cell Number':cell_num,'Aspect Ratio':aspect_ratios,'Body Intensity Sum':body_sums,'Body Intensity':body_means,'Cortex Intensity':cortex_means,'Leading Edge Intensity':lead_means,'Side & Rear Intensity':side_rear_means,'Cortex_Body':cortex_body,'Leading Edge_Body':lead_body,'SideRear_Body':side_rear_body,'Leading Edge_SideRear':lead_side_rear,'Filo Number':final_assignments,'Filos/10um':filoDensity,'Cell Area (um^2)':areas,'Perimeter (um)':perimeters,'Average Inter-filo Distance (um)':space_cell_avg}\n",
        "  else:\n",
        "    cell_dict={'Experiment Name':exp_num_cell,'Cell Number':cell_num,'Aspect Ratio':aspect_ratios,'Body Intensity Sum':body_sums,'Body Intensity':body_means,'Cortex Intensity':cortex_means,'Leading Edge Intensity':lead_means,'Side & Rear Intensity':side_rear_means,'Cortex_Body':cortex_body,'Leading Edge_Body':lead_body,'SideRear_Body':side_rear_body,'Leading Edge_SideRear':lead_side_rear,'Filo Number':final_assignments,'Filos/10um':filoDensity,'Cell Area (um^2)':areas,'Perimeter (um)':perimeters}\n",
        "  filo_dict={'Experiment Name':exp_num_filo,'Filo Number':filo_num,'Cell Assignment':cell_assignment,'Tip Intensity Sum':filo_sums,'Filo Tip Intensity':filo_means,'Assigned Cell Body Intensity':filo_cell_vals,'Filo Tip Area (um^2)':filo_areas,'Filo/Body':filo_body,'Filo Length(um)':filo_lengths_um}\n",
        "  cell_output=pd.DataFrame(cell_dict)\n",
        "  filo_output=pd.DataFrame(filo_dict)\n",
        "  cell_output.to_csv('filoTips_Output/'+str(images[q])+'_Cell_Output_.csv',index=False)\n",
        "  filo_output.to_csv('filoTips_Output/'+str(images[q])+'_Filo_Output_.csv',index=False)\n",
        "\n",
        "  if filoSpace==True:\n",
        "    fig, final=plt.subplots(1,3)\n",
        "    final[0].imshow(img)\n",
        "    final[0].set_title('Original Image')\n",
        "    final[0].axis('off')\n",
        "    final[1].imshow(img_copy3)\n",
        "    final[1].set_title('filoTips')\n",
        "    final[1].axis('off')\n",
        "    final[2].imshow(img_copy5)\n",
        "    final[2].set_title('filoSpace')\n",
        "    final[2].axis('off')\n",
        "    plt.savefig('filoTips_Output/'+str(images[q])+'_Annotation.tiff',dpi=Annotation_DPI)\n",
        "  else:\n",
        "    fig, final=plt.subplots(1,2)\n",
        "    final[0].imshow(img)\n",
        "    final[0].set_title('Original Image')\n",
        "    final[0].axis('off')\n",
        "    final[1].imshow(img_copy3)\n",
        "    final[1].set_title('filoTips')\n",
        "    final[1].axis('off')\n",
        "    plt.savefig('filoTips_Output/'+str(images[q])+'_Annotation.tiff',dpi=Annotation_DPI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_GJCtkECUpH",
        "cellView": "form"
      },
      "source": [
        "#@title 6) Pool together the analysis\n",
        "os.chdir('/content')\n",
        "cell_outputs=sorted(glob.glob(\"filoTips_Output/*Cell_Output_.csv\"))\n",
        "filo_outputs=sorted(glob.glob('filoTips_Output/*Filo_Output_.csv'))\n",
        "cell_dfs = (pd.read_csv(f) for f in cell_outputs)\n",
        "cell_dfs = pd.concat(cell_dfs, ignore_index=True)\n",
        "filo_dfs = (pd.read_csv(f) for f in filo_outputs)\n",
        "filo_dfs = pd.concat(filo_dfs, ignore_index=True)\n",
        "cell_dfs.to_csv('filoTips_Output/Total_Cell_Output.csv',index=False)\n",
        "filo_dfs.to_csv('filoTips_Output/Total_Filo_Output.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7) Optional data visualization\n",
        "cell_file=pd.read_csv('filoTips_Output/Total_Cell_Output.csv')\n",
        "if not os.path.exists('filoTips_Output'):\n",
        "        os.makedirs('filoTips_Output')\n",
        "if not os.path.exists('filoTips_Output'):\n",
        "        os.makedirs('filoTips_Output')\n",
        "if not os.path.exists('filoTips_Output/Plots'):\n",
        "        os.makedirs('filoTips_Output/Plots')\n",
        "\n",
        "if comparative_analysis ==True:\n",
        "  prompt1=[Condition_1,Condition_2,Condition_3,Condition_4]\n",
        "  if '' in prompt1:\n",
        "    prompt1.remove('')\n",
        "    if '' in prompt1:\n",
        "      prompt1.remove('')\n",
        "  prompt1=str(len(prompt1))\n",
        "  parameter=['Aspect Ratio','Cortex_Body','Leading Edge_Body','SideRear_Body','Leading Edge_Body','Filo Number','Cell Area (um^2)','Perimeter (um)']\n",
        "\n",
        "  data= cell_file\n",
        "  for t in range(0,len(parameter)):\n",
        "    if prompt1=='2':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "\n",
        "      print('-----Kruskal-Wallis Test-----')\n",
        "      kruskal=stats.kruskal(set1[parameter[t]],set2[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #t-test\n",
        "      print('-----T-test-----')\n",
        "      t_test=rp.ttest(group1=set1[parameter[t]],group1_name=Condition_1,\n",
        "              group2=set2[parameter[t]],group2_name=Condition_2)\n",
        "      print(t_test)\n",
        "      means=np.round([t_test[0]['Mean'][0],t_test[0]['Mean'][1]],decimals=2)\n",
        "      stds=np.round([t_test[0]['SD'][0],t_test[0]['SD'][1]],decimals=2)\n",
        "      bars=t_test[0]['Variable'][0],t_test[0]['Variable'][1]\n",
        "      x_pos=x_pos=list(np.arange(len(bars)))\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      plt.show()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      fig.savefig('filoTips_Output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')\n",
        "\n",
        "    if prompt1=='3':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          ind3=data.loc[data['Experiment Name'].str.contains(Condition_3)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "          data['Condition'][ind3]=Condition_3\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "      set3=data.loc[data['Condition'].str.contains(Condition_3)].reset_index(drop=True)\n",
        "\n",
        "      #kruskal-wallis\n",
        "      print('-----Kruskal-Wallis Test-----')\n",
        "      kruskal=stats.kruskal(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #ANOVA\n",
        "      print('-----ANOVA-----')\n",
        "      an=stats.f_oneway(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]])\n",
        "      vals=data[parameter[t]].tolist()\n",
        "      names=data['Condition'].tolist()\n",
        "      tukey=pairwise_tukeyhsd(endog=vals,\n",
        "                              groups=names,\n",
        "                              alpha=0.05)\n",
        "      print(tukey)\n",
        "      means=np.round([np.mean(set1[parameter[t]]),np.mean(set2[parameter[t]]),np.mean(set3[parameter[t]])],decimals=2)\n",
        "      stds=np.round([np.std(set1[parameter[t]]),np.std(set2[parameter[t]]),np.std(set3[parameter[t]])],decimals=2)\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]]),len(set3[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5],palp[1]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2,Condition_3], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      plt.show()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      fig.savefig('filoTips_Output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')\n",
        "\n",
        "\n",
        "    if prompt1=='4':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          ind3=data.loc[data['Experiment Name'].str.contains(Condition_3)].index\n",
        "          ind4=data.loc[data['Experiment Name'].str.contains(Condition_4)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "          data['Condition'][ind3]=Condition_3\n",
        "          data['Condition'][ind4]=Condition_4\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "      set3=data.loc[data['Condition'].str.contains(Condition_3)].reset_index(drop=True)\n",
        "      set4=data.loc[data['Condition'].str.contains(Condition_4)].reset_index(drop=True)\n",
        "\n",
        "      #kruskal-wallis\n",
        "      print('-----Kruskal-Wallis Test-----')\n",
        "      kruskal=stats.kruskal(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]],set4[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #ANOVA\n",
        "      print('-----ANOVA-----')\n",
        "      an=stats.f_oneway(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]],set4[parameter[t]])\n",
        "      vals=data[parameter[t]].tolist()\n",
        "      names=data['Condition'].tolist()\n",
        "      tukey=pairwise_tukeyhsd(endog=vals,\n",
        "                              groups=names,\n",
        "                              alpha=0.05)\n",
        "      print(tukey)\n",
        "      means=np.round([np.mean(set1[parameter[t]]),np.mean(set2[parameter[t]]),np.mean(set3[parameter[t]]),np.mean(set4[parameter[t]])],decimals=2)\n",
        "      stds=np.round([np.std(set1[parameter[t]]),np.std(set2[parameter[t]]),np.std(set3[parameter[t]]),np.std(set4[parameter[t]])],decimals=2)\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]]),len(set3[parameter[t]]),len(set4[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5],palp[1],palp[7]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2,Condition_3,Condition_4], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      plt.show()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      fig.savefig('filoTips_Output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nCQt9PSoDvAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK27w1Gqn9rt",
        "cellView": "form"
      },
      "source": [
        "#@title 8) Download filoTips_Output locally\n",
        "os.chdir('/content/filoTips_Output')\n",
        "if not os.path.exists('Annotations'):\n",
        "        os.makedirs('Annotations')\n",
        "if not os.path.exists('Individual Experiments'):\n",
        "        os.makedirs('Individual Experiments')\n",
        "\n",
        "annot_files=glob.glob('*.tiff')\n",
        "ind_files=glob.glob('*_.csv')\n",
        "\n",
        "for i in ind_files:\n",
        "  os.replace(i,'Individual Experiments/'+i)\n",
        "for i in annot_files:\n",
        "  os.replace(i,'Annotations/'+i)\n",
        "\n",
        "os.chdir('/content')\n",
        "if download_predictions == True:\n",
        "  shutil.move('/content/filoTips predictions','/content/filoTips_Output/filoTips predictions')\n",
        "\n",
        "os.chdir('/content')\n",
        "shutil.make_archive(output_filename, 'zip', 'filoTips_Output')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_filename+'.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}