{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# filoSkeleton Instructions:\n",
        "\n",
        "1) Collect images of your cells making filopodia\n",
        "*   Image of cell marker clearly highlighting both the cell body and filopodia stalks (8-bit tif)\n",
        "*   Image of filopodia tip marker (8-bit tif)\n",
        "*   Merged cell and filopodia tip image depicting cell body, filopodia stalks, and filopodia tips (RGB tif)\n",
        "\n",
        "2) Make sure all images are named according to the filoSkeleton format\n",
        "*   Please use the extension \".tif\" not \".tiff\"\n",
        "*   The cell body, filopodia, and merged images must have the same source-name (\"02-02-21_condition_cell_1.tif\")   \n",
        "*   Cell body images should include the extension \"_actin.tif\" (\"02-02-21_condition_cell_1_actin.tif\")\n",
        "*   Filopodia tip images should include the extension \"_filotips.tif\" (\"02-02-21_condition_cell_1_filotips.tif\")\n",
        "*   Merged images should include the source-name with the extension \"_RGB.tif\" (\"02-02-21_condition_cell_1_RGB.tif\")\n",
        "\n",
        "3) Describe the images using the variables below\n",
        "*   output_filename: Name of the experiment (\"02-02-21 data collection\")\n",
        "*   actin_channel: Provide extension for the actin marker *Must be present in all actin marker images (\"actin\")\n",
        "*   filo_channel: Provide extension for the filopodia tip marker *Must be present in all filopodia tip marker images (\"filotips\")\n",
        "*   filo_name: Name of the filopodia tip marker (\"Myo10\")\n",
        "*   um_per_pixel: Input the micron/pixel ratio *Must enter only the number (\"0.05\")\n",
        "*   use_default_cell_model: Would you like to use the default filoSkeleton cell DL model or a custom model trained via a ZeroCostDL4Mic notebook?\n",
        "*   custom_cell_model_DriveLink: If using a custom model - provide the Google Drive link to the compressed .zip file here (Right-click > Share > Share > General access > Anyone with link > Copy Link > Paste link here)\n",
        "*   use_default_filotip_model: Would you like to use the default filoSkeleton filopodia tip DL model or a custom model trained via a ZeroCostDL4Mic notebook?\n",
        "*   custom_filotip_model_DriveLink: If using a custom model - provide the Google Drive link to the compressed .zip file here (Right-click > Share > Share > General access > Anyone with link > Copy Link > Paste link here)\n",
        "*   comparative_analysis: Are you directly comparing two or more conditions? *If so, need to provide condition strings\n",
        "*   condition1-4: List the conditions you are comparing *Condition strings provided must be in the name of the images associated with each condition - Only required if comparison is selected\n",
        "*   filoSpace: Do you want interfilopodial spacing information? *Must have high enough resolution\n",
        "4) Drag and drop the images into the \"Files\" dropbox on the left of the Google Colab Notebook\n",
        "\n",
        "5) Click \"RunTime\" and then \"Run all\"\n",
        "\n",
        "6) Once filoSkeleton is done, a folder containing the analysis will be downloaded locally to your system\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YKU_92D2YnZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Provide information describing the data and load ZeroCostDL4Mic requirements (code modified slightly from \"U-Net_2D_Multilabel\" notebook)\n",
        "#code from ZeroCostDL4Mic 1.1\n",
        "from __future__ import print_function\n",
        "import os\n",
        "!pip install data\n",
        "!pip install fpdf\n",
        "!pip install h5py==2.10\n",
        "!pip install imagecodecs\n",
        "\n",
        "#code from ZeroCostDL4Mic 1.3\n",
        "Notebook_version = '1.13'\n",
        "Network = 'U-Net (2D) multilabel'\n",
        "\n",
        "import imagecodecs\n",
        "from builtins import any as b_any\n",
        "\n",
        "def get_requirements_path():\n",
        "    # Store requirements file in 'contents' directory\n",
        "    current_dir = os.getcwd()\n",
        "    dir_count = current_dir.count('/') - 1\n",
        "    path = '../' * (dir_count) + 'requirements.txt'\n",
        "    return path\n",
        "\n",
        "def filter_files(file_list, filter_list):\n",
        "    filtered_list = []\n",
        "    for fname in file_list:\n",
        "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
        "            filtered_list.append(fname)\n",
        "    return filtered_list\n",
        "\n",
        "def build_requirements_file(before, after):\n",
        "    path = get_requirements_path()\n",
        "\n",
        "    # Exporting requirements.txt for local run\n",
        "    !pip freeze > $path\n",
        "\n",
        "    # Get minimum requirements file\n",
        "    #df = pd.read_csv(path, delimiter = \"\\n\")\n",
        "    df = pd.read_csv(path)\n",
        "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
        "    req_list_temp = df.values.tolist()\n",
        "    req_list = [x[0] for x in req_list_temp]\n",
        "\n",
        "    # Replace with package name and handle cases where import name is different to module name\n",
        "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
        "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
        "    filtered_list = filter_files(req_list, mod_replace_list)\n",
        "\n",
        "    file=open(path,'w')\n",
        "    for item in filtered_list:\n",
        "        file.writelines(item + '\\n')\n",
        "\n",
        "    file.close()\n",
        "\n",
        "import sys\n",
        "before = [str(m) for m in sys.modules]\n",
        "\n",
        "#@markdown\n",
        "output_filename = \"filoSkeleton\" #@param {type:\"string\"}\n",
        "actin_channel = \"GFP\" #@param [\"actin\",\"GFP\", \"Cherry\"] {allow-input: true}\n",
        "filo_channel = \"CY5\" #@param [\"filotips\",\"mCherry\", \"Cherry\", \"CY5\"] {allow-input: true}\n",
        "filo_name = \"Myo10\" #@param {type:\"string\"}\n",
        "um_per_pixel =  0.11#@param {type:\"number\"}\n",
        "use_default_cell_model = True #@param {type:\"boolean\"}\n",
        "custom_cell_model_DriveLink= 'https://drive.google.com/file/d/1wnCJAVDNxd9pSpUc6KgXppig3N-MKExD/view?usp=drive_link' #@param {type:\"string\"}\n",
        "temp = custom_cell_model_DriveLink.replace('https://drive.google.com/file/d/','')\n",
        "temp = temp.replace('/view?usp=drive_link','')\n",
        "temp = temp.replace('/view?usp=sharing','')\n",
        "custom_cell_model_DriveLink = temp\n",
        "custom_cell_model_name= '' #@param {type:\"string\"}\n",
        "use_default_filotip_model = True #@param {type:\"boolean\"}\n",
        "custom_filotip_model_DriveLink= 'https://drive.google.com/file/d/1UFCtMLfV8PP1eX2imUa7aQg0HRbtlKcw/view?usp=drive_link' #@param {type:\"string\"}\n",
        "temp = custom_filotip_model_DriveLink.replace('https://drive.google.com/file/d/','')\n",
        "temp = temp.replace('/view?usp=drive_link','')\n",
        "temp = temp.replace('/view?usp=sharing','')\n",
        "custom_filotip_model_DriveLink = temp\n",
        "Annotation_DPI = 600 #@param {type:\"number\"}\n",
        "\n",
        "comparative_analysis = False #@param {type:\"boolean\"}\n",
        "Condition_1 = \"\" #@param {type:\"string\"}\n",
        "Condition_2 = \"\" #@param {type:\"string\"}\n",
        "Condition_3 = \"\" #@param {type:\"string\"}\n",
        "Condition_4 = \"\" #@param {type:\"string\"}\n",
        "filoSpace = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "pixel_micron=1/um_per_pixel\n",
        "#As this notebokk depends mostly on keras which runs a tensorflow backend (which in turn is pre-installed in colab)\n",
        "#only the data library needs to be additionally installed.\n",
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "# print(tensorflow.__version__)\n",
        "# print(\"Tensorflow enabled.\")\n",
        "\n",
        "\n",
        "# Keras imports\n",
        "from keras import models\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger # we currently don't use any other callbacks from ModelCheckpoints\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras import backend as keras\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "# General import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from skimage import img_as_ubyte, io, transform\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import sys\n",
        "from math import ceil\n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from pip._internal.operations.freeze import freeze\n",
        "import subprocess\n",
        "# Imports for QC\n",
        "from PIL import Image\n",
        "from scipy import signal\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from skimage.util import img_as_uint\n",
        "from skimage.metrics import structural_similarity\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# For sliders and dropdown menu and progress bar\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.feature_extraction import image\n",
        "from skimage import img_as_ubyte, io, transform\n",
        "from skimage.util.shape import view_as_windows\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Suppressing some warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_patches(Training_source, Training_target, patch_width, patch_height, min_fraction):\n",
        "  \"\"\"\n",
        "  Function creates patches from the Training_source and Training_target images.\n",
        "  The steps parameter indicates the offset between patches and, if integer, is the same in x and y.\n",
        "  Saves all created patches in two new directories in the /content folder.\n",
        "\n",
        "  Returns: - Two paths to where the patches are now saved\n",
        "  \"\"\"\n",
        "  DEBUG = False\n",
        "\n",
        "  Patch_source = os.path.join('/content','img_patches')\n",
        "  Patch_target = os.path.join('/content','mask_patches')\n",
        "  Patch_rejected = os.path.join('/content','rejected')\n",
        "\n",
        "  #Here we save the patches, in the /content directory as they will not usually be needed after training\n",
        "  if os.path.exists(Patch_source):\n",
        "    shutil.rmtree(Patch_source)\n",
        "  if os.path.exists(Patch_target):\n",
        "    shutil.rmtree(Patch_target)\n",
        "  if os.path.exists(Patch_rejected):\n",
        "    shutil.rmtree(Patch_rejected)\n",
        "\n",
        "  os.mkdir(Patch_source)\n",
        "  os.mkdir(Patch_target)\n",
        "  os.mkdir(Patch_rejected) #This directory will contain the images that have too little signal.\n",
        "\n",
        "  patch_num = 0\n",
        "\n",
        "  for file in tqdm(os.listdir(Training_source)):\n",
        "\n",
        "    img = io.imread(os.path.join(Training_source, file))\n",
        "    mask = io.imread(os.path.join(Training_target, file),as_gray=True)\n",
        "\n",
        "    if DEBUG:\n",
        "      print(file)\n",
        "      print(img.dtype)\n",
        "\n",
        "    # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n",
        "    patches_img = view_as_windows(img, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n",
        "\n",
        "    patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width,patch_height)\n",
        "    patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width,patch_height)\n",
        "\n",
        "    if DEBUG:\n",
        "      print(all_patches_img.shape)\n",
        "      print(all_patches_img.dtype)\n",
        "\n",
        "    for i in range(patches_img.shape[0]):\n",
        "      img_save_path = os.path.join(Patch_source,'patch_'+str(patch_num)+'.tif')\n",
        "      mask_save_path = os.path.join(Patch_target,'patch_'+str(patch_num)+'.tif')\n",
        "      patch_num += 1\n",
        "\n",
        "      # if the mask conatins at least 2% of its total number pixels as mask, then go ahead and save the images\n",
        "      pixel_threshold_array = sorted(patches_mask[i].flatten())\n",
        "      if pixel_threshold_array[int(round((len(pixel_threshold_array)-1)*(1-min_fraction)))]>0:\n",
        "        io.imsave(img_save_path, img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
        "        io.imsave(mask_save_path, patches_mask[i])\n",
        "      else:\n",
        "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_image.tif', img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
        "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_mask.tif', patches_mask[i])\n",
        "\n",
        "  return Patch_source, Patch_target\n",
        "\n",
        "\n",
        "def estimatePatchSize(data_path, max_width = 512, max_height = 512):\n",
        "\n",
        "  files = os.listdir(data_path)\n",
        "\n",
        "  # Get the size of the first image found in the folder and initialise the variables to that\n",
        "  n = 0\n",
        "  while os.path.isdir(os.path.join(data_path, files[n])):\n",
        "    n += 1\n",
        "  (height_min, width_min) = Image.open(os.path.join(data_path, files[n])).size\n",
        "\n",
        "  # Screen the size of all dataset to find the minimum image size\n",
        "  for file in files:\n",
        "    if not os.path.isdir(os.path.join(data_path, file)):\n",
        "      (height, width) = Image.open(os.path.join(data_path, file)).size\n",
        "      if width < width_min:\n",
        "        width_min = width\n",
        "      if height < height_min:\n",
        "        height_min = height\n",
        "\n",
        "  # Find the power of patches that will fit within the smallest dataset\n",
        "  width_min, height_min = (fittingPowerOfTwo(width_min), fittingPowerOfTwo(height_min))\n",
        "\n",
        "  # Clip values at maximum permissible values\n",
        "  if width_min > max_width:\n",
        "    width_min = max_width\n",
        "\n",
        "  if height_min > max_height:\n",
        "    height_min = max_height\n",
        "\n",
        "  return (width_min, height_min)\n",
        "\n",
        "def fittingPowerOfTwo(number):\n",
        "  n = 0\n",
        "  while 2**n <= number:\n",
        "    n += 1\n",
        "  return 2**(n-1)\n",
        "\n",
        "## TODO: create weighted CE for semantic labels\n",
        "def getClassWeights(Training_target_path):\n",
        "\n",
        "  Mask_dir_list = os.listdir(Training_target_path)\n",
        "  number_of_dataset = len(Mask_dir_list)\n",
        "\n",
        "  class_count = np.zeros(2, dtype=int)\n",
        "  for i in tqdm(range(number_of_dataset)):\n",
        "    mask = io.imread(os.path.join(Training_target_path, Mask_dir_list[i]))\n",
        "    mask = normalizeMinMax(mask)\n",
        "    class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "    class_count[1] += mask.sum()\n",
        "\n",
        "  n_samples = class_count.sum()\n",
        "  n_classes = 2\n",
        "\n",
        "  class_weights = n_samples / (n_classes * class_count)\n",
        "  return class_weights\n",
        "\n",
        "def weighted_binary_crossentropy(class_weights):\n",
        "\n",
        "    def _weighted_binary_crossentropy(y_true, y_pred):\n",
        "        binary_crossentropy = keras.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * class_weights[1] + (1. - y_true) * class_weights[0]\n",
        "        weighted_binary_crossentropy = weight_vector * binary_crossentropy\n",
        "\n",
        "        return keras.mean(weighted_binary_crossentropy)\n",
        "\n",
        "    return _weighted_binary_crossentropy\n",
        "\n",
        "\n",
        "def save_augment(datagen,orig_img,dir_augmented_data=\"/content/augment\"):\n",
        "  \"\"\"\n",
        "  Saves a subset of the augmented data for visualisation, by default in /content.\n",
        "\n",
        "  This is adapted from: https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    os.mkdir(dir_augmented_data)\n",
        "  except:\n",
        "        ## if the preview folder exists, then remove\n",
        "        ## the contents (pictures) in the folder\n",
        "    for item in os.listdir(dir_augmented_data):\n",
        "      os.remove(dir_augmented_data + \"/\" + item)\n",
        "\n",
        "    ## convert the original image to array\n",
        "  x = img_to_array(orig_img)\n",
        "    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n",
        "    #print(x.shape)\n",
        "  x = x.reshape((1,) + x.shape)\n",
        "    #print(x.shape)\n",
        "    ## -------------------------- ##\n",
        "    ## randomly generate pictures\n",
        "    ## -------------------------- ##\n",
        "  i = 0\n",
        "    #We will just save 5 images,\n",
        "    #but this can be changed, but note the visualisation in 3. currently uses 5.\n",
        "  Nplot = 5\n",
        "  for batch in datagen.flow(x,batch_size=1,\n",
        "                            save_to_dir=dir_augmented_data,\n",
        "                            save_format='tif',\n",
        "                            seed=42):\n",
        "    i += 1\n",
        "    if i > Nplot - 1:\n",
        "      break\n",
        "\n",
        "# Generators\n",
        "def buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, subset, batch_size, target_size, validatio_split):\n",
        "  '''\n",
        "  Can generate image and mask at the same time use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
        "\n",
        "  datagen: ImageDataGenerator\n",
        "  subset: can take either 'training' or 'validation'\n",
        "  '''\n",
        "\n",
        "  # Build the dict for the ImageDataGenerator\n",
        "  # non_aug_args = dict(width_shift_range = 0,\n",
        "  #                     height_shift_range = 0,\n",
        "  #                     rotation_range = 0, #90\n",
        "  #                     zoom_range = 0,\n",
        "  #                     shear_range = 0,\n",
        "  #                     horizontal_flip = False,\n",
        "  #                     vertical_flip = False,\n",
        "  #                     fill_mode = 'reflect')\n",
        "  # default params of data generator is without augmentation\n",
        "  mask_load_gen = ImageDataGenerator(dtype='uint8', validation_split=validatio_split)\n",
        "  image_load_gen = ImageDataGenerator(dtype='float32', validation_split=validatio_split, preprocessing_function = normalizePercentile)\n",
        "\n",
        "  image_generator = image_load_gen.flow_from_directory(\n",
        "        os.path.dirname(image_folder_path),\n",
        "        classes = [os.path.basename(image_folder_path)],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        subset = subset,\n",
        "        interpolation = \"bicubic\",\n",
        "        seed = 1)\n",
        "  mask_generator = mask_load_gen.flow_from_directory(\n",
        "        os.path.dirname(mask_folder_path),\n",
        "        classes = [os.path.basename(mask_folder_path)],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        subset = subset,\n",
        "        interpolation = \"nearest\",\n",
        "        seed = 1)\n",
        "\n",
        "  this_generator = zip(image_generator, mask_generator)\n",
        "  for (img,mask) in this_generator:\n",
        "      if subset == 'training':\n",
        "          # Apply the data augmentation\n",
        "          # the same seed should provide always the same transformation and image loading\n",
        "          seed = np.random.randint(100000)\n",
        "          for batch_im in image_datagen.flow(img,batch_size=batch_size, seed=seed):\n",
        "              break\n",
        "          mask = mask.astype(np.float32)\n",
        "          labels = np.unique(mask)\n",
        "          if len(labels)>1:\n",
        "              batch_mask = np.zeros_like(mask, dtype='float32')\n",
        "              for l in range(0, len(labels)):\n",
        "                  aux = (mask==l).astype(np.float32)\n",
        "                  for batch_aux in mask_datagen.flow(aux,batch_size=batch_size, seed=seed):\n",
        "                      break\n",
        "                  batch_mask += l*(batch_aux>0).astype(np.float32)\n",
        "              index = np.where(batch_mask>l)\n",
        "              batch_mask[index]=l\n",
        "          else:\n",
        "              batch_mask = mask\n",
        "\n",
        "          yield (batch_im,batch_mask)\n",
        "\n",
        "      else:\n",
        "          yield (img,mask)\n",
        "\n",
        "\n",
        "def prepareGenerators(image_folder_path, mask_folder_path, datagen_parameters, batch_size = 4, target_size = (512, 512), validatio_split = 0.1):\n",
        "  image_datagen = ImageDataGenerator(**datagen_parameters, preprocessing_function = normalizePercentile)\n",
        "  mask_datagen = ImageDataGenerator(**datagen_parameters)\n",
        "\n",
        "  train_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'training', batch_size, target_size, validatio_split)\n",
        "  validation_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'validation', batch_size, target_size, validatio_split)\n",
        "\n",
        "  return (train_datagen, validation_datagen)\n",
        "\n",
        "\n",
        "# Normalization functions from Martin Weigert\n",
        "def normalizePercentile(x, pmin=1, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    \"\"\"Percentile-based image normalization.\"\"\"\n",
        "\n",
        "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
        "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
        "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
        "\n",
        "\n",
        "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    if dtype is not None:\n",
        "        x   = x.astype(dtype,copy=False)\n",
        "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
        "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
        "        eps = dtype(eps)\n",
        "\n",
        "    try:\n",
        "        import numexpr\n",
        "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
        "    except ImportError:\n",
        "        x =                   (x - mi) / ( ma - mi + eps )\n",
        "\n",
        "    if clip:\n",
        "        x = np.clip(x,0,1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "# Simple normalization to min/max fir the Mask\n",
        "def normalizeMinMax(x, dtype=np.float32):\n",
        "  x = x.astype(dtype,copy=False)\n",
        "  x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
        "  return x\n",
        "\n",
        "\n",
        "# This is code outlines the architecture of U-net. The choice of pooling steps decides the depth of the network.\n",
        "def unet(pretrained_weights = None, input_size = (256,256,1), pooling_steps = 4, learning_rate = 1e-4, verbose=True, labels=2):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    # Downsampling steps\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "\n",
        "    if pooling_steps > 1:\n",
        "      pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "\n",
        "      if pooling_steps > 2:\n",
        "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "        drop4 = Dropout(0.5)(conv4)\n",
        "\n",
        "        if pooling_steps > 3:\n",
        "          pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "          drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "          #Upsampling steps\n",
        "          up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "          merge6 = concatenate([drop4,up6], axis = 3)\n",
        "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    if pooling_steps > 2:\n",
        "      up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n",
        "      if pooling_steps > 3:\n",
        "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "      merge7 = concatenate([conv3,up7], axis = 3)\n",
        "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    if pooling_steps > 1:\n",
        "      up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv3))\n",
        "      if pooling_steps > 2:\n",
        "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "      merge8 = concatenate([conv2,up8], axis = 3)\n",
        "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    if pooling_steps == 1:\n",
        "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "    else:\n",
        "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8)) #activation = 'relu'\n",
        "\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9) #activation = 'relu'\n",
        "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
        "    conv9 = Conv2D(labels, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
        "    conv10 = Conv2D(labels, 1, activation = 'softmax')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = learning_rate), loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "    if verbose:\n",
        "      model.summary()\n",
        "\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights);\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom callback showing sample prediction\n",
        "class SampleImageCallback(Callback):\n",
        "\n",
        "    def __init__(self, model, sample_data, model_path, save=False):\n",
        "        self.model = model\n",
        "        self.sample_data = sample_data\n",
        "        self.model_path = model_path\n",
        "        self.save = save\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      if np.mod(epoch,5) == 0:\n",
        "            sample_predict = self.model.predict_on_batch(self.sample_data)\n",
        "\n",
        "            f=plt.figure(figsize=(16,8))\n",
        "            plt.subplot(1,labels+1,1)\n",
        "            plt.imshow(self.sample_data[0,:,:,0], cmap='gray')\n",
        "            plt.title('Sample source')\n",
        "            plt.axis('off');\n",
        "            for i in range(1, labels):\n",
        "              plt.subplot(1,labels+1,i+1)\n",
        "              plt.imshow(sample_predict[0,:,:,i], interpolation='nearest', cmap='magma')\n",
        "              plt.title('Predicted label {}'.format(i))\n",
        "              plt.axis('off');\n",
        "\n",
        "            plt.subplot(1,labels+1,labels+1)\n",
        "            plt.imshow(np.squeeze(np.argmax(sample_predict[0], axis=-1)), interpolation='nearest')\n",
        "            plt.title('Semantic segmentation')\n",
        "            plt.axis('off');\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            if self.save:\n",
        "                plt.savefig(self.model_path + '/epoch_' + str(epoch+1) + '.png')\n",
        "                random_choice = random.choice(os.listdir(Patch_source))\n",
        "\n",
        "def predict_as_tiles(Image_path, model):\n",
        "\n",
        "  # Read the data in and normalize\n",
        "  Image_raw = io.imread(Image_path, as_gray = True)\n",
        "  Image_raw = normalizePercentile(Image_raw)\n",
        "\n",
        "  # Get the patch size from the input layer of the model\n",
        "  #patch_size = model.layers[0].output_shape[1:3]\n",
        "  patch_size = model.layers[0].output_shape[0][1:3]\n",
        "\n",
        "  # Pad the image with zeros if any of its dimensions is smaller than the patch size\n",
        "  if Image_raw.shape[0] < patch_size[0] or Image_raw.shape[1] < patch_size[1]:\n",
        "    Image = np.zeros((max(Image_raw.shape[0], patch_size[0]), max(Image_raw.shape[1], patch_size[1])))\n",
        "    Image[0:Image_raw.shape[0], 0: Image_raw.shape[1]] = Image_raw\n",
        "  else:\n",
        "    Image = Image_raw\n",
        "\n",
        "  # Calculate the number of patches in each dimension\n",
        "  n_patch_in_width = ceil(Image.shape[0]/patch_size[0])\n",
        "  n_patch_in_height = ceil(Image.shape[1]/patch_size[1])\n",
        "\n",
        "  prediction = np.zeros(Image.shape, dtype = 'uint8')\n",
        "\n",
        "  for x in range(n_patch_in_width):\n",
        "    for y in range(n_patch_in_height):\n",
        "      xi = patch_size[0]*x\n",
        "      yi = patch_size[1]*y\n",
        "\n",
        "      # If the patch exceeds the edge of the image shift it back\n",
        "      if xi+patch_size[0] >= Image.shape[0]:\n",
        "        xi = Image.shape[0]-patch_size[0]\n",
        "\n",
        "      if yi+patch_size[1] >= Image.shape[1]:\n",
        "        yi = Image.shape[1]-patch_size[1]\n",
        "\n",
        "      # Extract and reshape the patch\n",
        "      patch = Image[xi:xi+patch_size[0], yi:yi+patch_size[1]]\n",
        "      patch = np.reshape(patch,patch.shape+(1,))\n",
        "      patch = np.reshape(patch,(1,)+patch.shape)\n",
        "\n",
        "      # Get the prediction from the patch and paste it in the prediction in the right place\n",
        "      predicted_patch = model.predict(patch, batch_size = 1)\n",
        "      prediction[xi:xi+patch_size[0], yi:yi+patch_size[1]] = (np.argmax(np.squeeze(predicted_patch), axis = -1)).astype(np.uint8)\n",
        "\n",
        "\n",
        "  return prediction[0:Image_raw.shape[0], 0: Image_raw.shape[1]]\n",
        "\n",
        "\n",
        "def saveResult(save_path, nparray, source_dir_list, prefix=''):\n",
        "  for (filename, image) in zip(source_dir_list, nparray):\n",
        "      io.imsave(os.path.join(save_path, prefix+os.path.splitext(filename)[0]+'.tif'), image) # saving as unsigned 8-bit image\n",
        "\n",
        "\n",
        "def convert2Mask(image, threshold):\n",
        "  mask = img_as_ubyte(image, force_copy=True)\n",
        "  mask[mask > threshold] = 255\n",
        "  mask[mask <= threshold] = 0\n",
        "  return mask\n",
        "\n",
        "# -------------- Other definitions -----------\n",
        "W  = '\\033[0m'  # white (normal)\n",
        "R  = '\\033[31m' # red\n",
        "prediction_prefix = 'Predicted_'\n",
        "\n",
        "\n",
        "print('-------------------')\n",
        "print('U-Net and dependencies installed.')\n",
        "\n",
        "# Colors for the warning messages\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "\n",
        "#All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
        "#print('Notebook version: '+Notebook_version)\n",
        "#Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
        "#print('Latest notebook version: '+Latest_Notebook_version)\n",
        "#if Notebook_version == Latest_Notebook_version:\n",
        "#  print(\"This notebook is up-to-date.\")\n",
        "#else:\n",
        "#  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
        "\n",
        "\n",
        "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "  # add another cell\n",
        "  if trained:\n",
        "    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
        "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
        "  pdf.ln(1)\n",
        "\n",
        "  Header_2 = 'Information for your materials and method:'\n",
        "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "  #print(all_packages)\n",
        "\n",
        "  #Main Packages\n",
        "  main_packages = ''\n",
        "  version_numbers = []\n",
        "  for name in ['tensorflow','numpy','Keras']:\n",
        "    find_name=all_packages.find(name)\n",
        "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
        "    #Version numbers only here:\n",
        "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
        "\n",
        "  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
        "  cuda_version = cuda_version.stdout.decode('utf-8')\n",
        "  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
        "  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
        "  gpu_name = gpu_name.stdout.decode('utf-8')\n",
        "  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
        "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
        "  #print(gpu_name)\n",
        "  loss = str(model.loss)[str(model.loss).find('function')+len('function'):str(model.loss).find('.<')]\n",
        "  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
        "  dataset_size = len(os.listdir(Training_source))\n",
        "\n",
        "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+' and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  if pretrained_model:\n",
        "    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+'  and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  pdf.multi_cell(180, 5, txt = text, align='L')\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(28, 5, txt='Augmentation: ', ln=1)\n",
        "  pdf.set_font('')\n",
        "  if augmentation:\n",
        "    aug_text = 'The dataset was augmented by'\n",
        "    if rotation_range != 0:\n",
        "      aug_text = aug_text+'\\n- rotation'\n",
        "    if horizontal_flip == True or vertical_flip == True:\n",
        "      aug_text = aug_text+'\\n- flipping'\n",
        "    if zoom_range != 0:\n",
        "      aug_text = aug_text+'\\n- random zoom magnification'\n",
        "    if horizontal_shift != 0 or vertical_shift != 0:\n",
        "      aug_text = aug_text+'\\n- shifting'\n",
        "    if shear_range != 0:\n",
        "      aug_text = aug_text+'\\n- image shearing'\n",
        "  else:\n",
        "    aug_text = 'No augmentation was used for training.'\n",
        "  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  if Use_Default_Advanced_Parameters:\n",
        "    pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
        "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
        "  pdf.ln(1)\n",
        "  html = \"\"\"\n",
        "  <table width=40% style=\"margin-left:0px;\">\n",
        "    <tr>\n",
        "      <th width = 50% align=\"left\">Parameter</th>\n",
        "      <th width = 50% align=\"left\">Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>number_of_epochs</td>\n",
        "      <td width = 50%>{0}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>patch_size</td>\n",
        "      <td width = 50%>{1}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>batch_size</td>\n",
        "      <td width = 50%>{2}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>number_of_steps</td>\n",
        "      <td width = 50%>{3}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>percentage_validation</td>\n",
        "      <td width = 50%>{4}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>initial_learning_rate</td>\n",
        "      <td width = 50%>{5}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>pooling_steps</td>\n",
        "      <td width = 50%>{6}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td width = 50%>min_fraction</td>\n",
        "      <td width = 50%>{7}</td>\n",
        "  </table>\n",
        "  \"\"\".format(number_of_epochs, str(patch_width)+'x'+str(patch_height), batch_size, number_of_steps, percentage_validation, initial_learning_rate, pooling_steps, min_fraction)\n",
        "  pdf.write_html(html)\n",
        "\n",
        "  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(28, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
        "  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
        "  pdf.set_font('')\n",
        "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread('/content/TrainingDataExample_Unet2D.png').shape\n",
        "  pdf.image('/content/TrainingDataExample_Unet2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "  # if Use_Data_augmentation:\n",
        "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
        "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
        "  pdf.ln(3)\n",
        "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "  pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n",
        "\n",
        "  print('------------------------------')\n",
        "  print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
        "\n",
        "def qc_pdf_export():\n",
        "  class MyFPDF(FPDF, HTMLMixin):\n",
        "    pass\n",
        "\n",
        "  pdf = MyFPDF()\n",
        "  pdf.add_page()\n",
        "  pdf.set_right_margin(-1)\n",
        "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "  Network = 'Unet 2D'\n",
        "\n",
        "  day = datetime.now()\n",
        "  datetime_str = str(day)[0:10]\n",
        "\n",
        "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
        "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "\n",
        "  all_packages = ''\n",
        "  for requirement in freeze(local_only=True):\n",
        "    all_packages = all_packages+requirement+', '\n",
        "\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(2)\n",
        "  pdf.cell(190, 5, txt = 'Loss curves', ln=1, align='L')\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
        "  if os.path.exists(full_QC_model_path+'/Quality Control/lossCurvePlots.png'):\n",
        "    pdf.image(full_QC_model_path+'/Quality Control/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/12), h = round(exp_size[0]/3))\n",
        "  else:\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size=10)\n",
        "    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.',align='L')\n",
        "  pdf.ln(2)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 10, style = 'B')\n",
        "  pdf.ln(3)\n",
        "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
        "  pdf.ln(1)\n",
        "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
        "  pdf.image(full_QC_model_path+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font('Arial', size = 11, style = 'B')\n",
        "  pdf.ln(1)\n",
        "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  html = \"\"\"\n",
        "  <body>\n",
        "  <font size=\"10\" face=\"Courier New\" >\n",
        "  <table width=60% style=\"margin-left:0px;\">\"\"\"\n",
        "  with open(full_QC_model_path+'/Quality Control/QC_metrics_'+QC_model_name+'.csv', 'r') as csvfile:\n",
        "    metrics = csv.reader(csvfile)\n",
        "    header = next(metrics)\n",
        "    image = header[0]\n",
        "    IoU = header[-1]\n",
        "    header = \"\"\"\n",
        "    <tr>\n",
        "    <th width = 33% align=\"center\">{0}</th>\n",
        "    <th width = 33% align=\"center\">{1}</th>\n",
        "    </tr>\"\"\".format(image,IoU)\n",
        "    html = html+header\n",
        "    i=0\n",
        "    for row in metrics:\n",
        "      i+=1\n",
        "      image = row[0]\n",
        "      IoU = row[-1]\n",
        "      cells = \"\"\"\n",
        "        <tr>\n",
        "          <td width = 33% align=\"center\">{0}</td>\n",
        "          <td width = 33% align=\"center\">{1}</td>\n",
        "        </tr>\"\"\".format(image,str(round(float(IoU),3)))\n",
        "      html = html+cells\n",
        "    html = html+\"\"\"</body></table>\"\"\"\n",
        "\n",
        "  pdf.write_html(html)\n",
        "\n",
        "  pdf.ln(1)\n",
        "  pdf.set_font('')\n",
        "  pdf.set_font_size(10.)\n",
        "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
        "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "\n",
        "  pdf.ln(3)\n",
        "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
        "\n",
        "  pdf.set_font('Arial', size = 11, style='B')\n",
        "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "  pdf.output(full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "  print('------------------------------')\n",
        "  print('QC PDF report exported as '+full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "# Build requirements file for local run\n",
        "after = [str(m) for m in sys.modules]\n",
        "build_requirements_file(before, after)\n",
        "\n",
        "#code from ZeroCostDL4Mic 2.1\n",
        "\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.')\n",
        "  print('Did you change your runtime ?')\n",
        "  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "  !nvidia-smi\n",
        "\n",
        "# from tensorflow.python.client import device_lib\n",
        "# device_lib.list_local_devices()\n",
        "\n",
        "# print the tensorflow version\n",
        "print('Tensorflow version is ' + str(tf.__version__))"
      ],
      "metadata": {
        "id": "7eJBXBwMrtrK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Load filoSkeleton requirements\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import shutil\n",
        "import tifffile as tiff\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install researchpy\n",
        "import researchpy as rp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_distance(x2,x1,y2,y1):\n",
        "    return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
        "\n",
        "red,aqua,white,black,purple,gray1,gray2,gray3=(220,20,60),(127,255,212),(0,0,0),(255,255,255),(178,58,238),(128,128,128),(64,64,64),(32,32,32)\n",
        "white,yellow,blue,green,pink,red,orange,black,light_orange,gray4=(255,255,255),(250,250,0),(51,153,255),(0,204,0),(255,0,127),(255,51,51),(153,76,0),(0,0,0),(255,178,102),(160,160,160)\n",
        "#orange,purple=(255,125,64),(191,62,255)\n",
        "pink,beige,peacock,peachpuff1=(255,130,171),(245,245,220),(51,161,201),(161,161,161)\n",
        "random_color=[(222,184,135),(255,211,155),(238,197,145),(205,170,125),(139,115,85),(138,54,15),(138,51,36),(95,158,160),(152,245,255),(142,229,238)]\n",
        "\n"
      ],
      "metadata": {
        "id": "3XJCxeEbRpA2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Download the deep learning agent and prepare for predictions\n",
        "os.chdir('/content')\n",
        "\n",
        "if not os.path.exists('/content/filoSkeleton cell predictions'):\n",
        "  os.makedirs('/content/filoSkeleton cell predictions')\n",
        "if not os.path.exists('/content/filoSkeleton filopodia tip predictions'):\n",
        "  os.makedirs('/content/filoSkeleton filopodia tip predictions')\n",
        "if not os.path.exists('/content/filoSkeleton cell source'):\n",
        "  os.makedirs('/content/filoSkeleton cell source')\n",
        "if not os.path.exists('/content/filoSkeleton filopodia tip source'):\n",
        "  os.makedirs('/content/filoSkeleton filopodia tip source')\n",
        "if not os.path.exists('/content/filoSkeleton merged'):\n",
        "  os.makedirs('/content/filoSkeleton merged')\n",
        "\n",
        "os.chdir('/content')\n",
        "green_files=sorted(glob.glob('*'+actin_channel+'.tiff'))\n",
        "if len(green_files)==0:\n",
        "  green_files=sorted(glob.glob('*'+actin_channel+'.tif'))\n",
        "for file in green_files:\n",
        "  shutil.copy(file,'/content/filoSkeleton cell source/'+file)\n",
        "\n",
        "red_files=sorted(glob.glob('*'+filo_channel+'.tiff'))\n",
        "if len(red_files)==0:\n",
        "  red_files=sorted(glob.glob('*'+filo_channel+'.tif'))\n",
        "for file in red_files:\n",
        "  shutil.copy(file,'/content/filoSkeleton filopodia tip source/'+file)\n",
        "\n",
        "merge_files=sorted(glob.glob('*RGB.tiff'))\n",
        "if len(merge_files)==0:\n",
        "  merge_files=sorted(glob.glob('*RGB.tif'))\n",
        "for file in merge_files:\n",
        "  shutil.copy(file,'/content/filoSkeleton merged/'+file)\n",
        "\n",
        "#Sandra_Final\n",
        "os.chdir('/content')\n",
        "if use_default_cell_model == True:\n",
        "  subprocess.call(['gdown', '--id', '1wnCJAVDNxd9pSpUc6KgXppig3N-MKExD'])\n",
        "  !unzip '/content/Sandra_Final.zip'\n",
        "\n",
        "if use_default_cell_model ==False:\n",
        "  subprocess.run(['gdown', custom_cell_model_DriveLink])\n",
        "  zip1=glob.glob('*.zip')[0]\n",
        "  custom_cell_model_name = zip1.replace('.zip','')\n",
        "  !unzip $zip1\n",
        "  !rm $zip1\n",
        "\n",
        "\n",
        "#Jerry_Final_Myo10\n",
        "if use_default_filotip_model ==True:\n",
        "  subprocess.call(['gdown', '--id', '1UFCtMLfV8PP1eX2imUa7aQg0HRbtlKcw'])\n",
        "  !unzip '/content/Jerry_Final_Myo10.zip'\n",
        "\n",
        "if use_default_filotip_model==False:\n",
        "  subprocess.run(['gdown', custom_filotip_model_DriveLink])\n",
        "  zip2=glob.glob('*.zip')[0]\n",
        "  custom_filotip_model_name = zip2.replace('.zip','')\n",
        "  !unzip $zip2\n",
        "  !rm $zip2\n",
        "\n",
        "#make images 8-bit\n",
        "#os.chdir('/filoSkeleton cell source')\n",
        "#for file in green_files:\n",
        "#  img=cv2.imread(file,-1)\n",
        "#  img=cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "#  tiff.imsave(file,img)\n",
        "#os.chdir('/filoSkeleton filopodia tip source')\n",
        "#for file in red_files:\n",
        "#  img=cv2.imread(file,-1)\n",
        "#  img=cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "#  tiff.imsave(file,img)\n",
        "\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "oYBjRCndsApn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Have the model generate masks of cell bodies, filopodia, and background\n",
        "\n",
        "\n",
        "# ------------- Initial user input ------------\n",
        "Data_folder = ['/content/filoSkeleton cell source','/content/filoSkeleton filopodia tip source']\n",
        "Results_folder = ['/content/filoSkeleton cell predictions','/content/filoSkeleton filopodia tip predictions']\n",
        "Prediction_model_folder = [\"/content/Sandra_Final\",'/content/Jerry_Final_Myo10']\n",
        "if use_default_cell_model==False:\n",
        "  Prediction_model_folder[0]= '/content/'+custom_cell_model_name\n",
        "if use_default_filotip_model==False:\n",
        "  Prediction_model_folder[1]='/content/'+custom_filotip_model_name\n",
        "\n",
        "for y in range(0,len(Data_folder)):\n",
        "  Use_the_current_trained_model = False\n",
        "\n",
        "  #Here we find the loaded model name and parent path\n",
        "  Prediction_model_name = os.path.basename(Prediction_model_folder[y])\n",
        "  Prediction_model_path = os.path.dirname(Prediction_model_folder[y])\n",
        "\n",
        "\n",
        "  # ------------- Failsafes ------------\n",
        "  if (Use_the_current_trained_model):\n",
        "    print(\"Using current trained network\")\n",
        "    Prediction_model_name = model_name\n",
        "    Prediction_model_path = model_path\n",
        "\n",
        "  full_Prediction_model_path = os.path.join(Prediction_model_path, Prediction_model_name)\n",
        "  if os.path.exists(full_Prediction_model_path):\n",
        "    print(\"The \"+Prediction_model_name+\" network will be used.\")\n",
        "  else:\n",
        "    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "    print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
        "\n",
        "\n",
        "  # ------------- Prepare the model and run predictions ------------\n",
        "\n",
        "  # Load the model and prepare generator\n",
        "\n",
        "\n",
        "\n",
        "  unet = load_model(os.path.join(Prediction_model_path, Prediction_model_name, 'weights_best.hdf5'), custom_objects={'_weighted_binary_crossentropy': weighted_binary_crossentropy(np.ones(2))})\n",
        "  #Input_size = unet.layers[0].output_shape[1:3]\n",
        "  Input_size= unet.layers[0].output_shape[0][1:3]\n",
        "  print('Model input size: '+str(Input_size[0])+'x'+str(Input_size[1]))\n",
        "\n",
        "  # Create a list of sources\n",
        "  source_dir_list = os.listdir(Data_folder[y])\n",
        "  number_of_dataset = len(source_dir_list)\n",
        "  print('Number of dataset found in the folder: '+str(number_of_dataset))\n",
        "\n",
        "  predictions = []\n",
        "  for i in range(number_of_dataset):\n",
        "    predictions.append(predict_as_tiles(os.path.join(Data_folder[y], source_dir_list[i]), unet))\n",
        "    #predictions.append(prediction(os.path.join(Data_folder[y], source_dir_list[i]), os.path.join(Prediction_model_path, Prediction_model_name)))\n",
        "\n",
        "\n",
        "  # Save the results in the folder along with the masks according to the set threshold\n",
        "  saveResult(Results_folder[y], predictions, source_dir_list, prefix=prediction_prefix)\n",
        "\n",
        "\n",
        "  # ------------- For display ------------\n",
        "  print('--------------------------------------------------------------')\n",
        "#  os.chdir(Results_folder[y])\n",
        "#  files=sorted(glob.glob('*.tif'))\n",
        "  #for file in files:\n",
        "  #  name=file.replace('.tif','.tiff')\n",
        "  #  os.rename(file,name)\n",
        "#  os.chdir(path)\n",
        "\n",
        "#  def show_prediction_mask(file=os.listdir(Data_folder[y])):\n",
        "#\n",
        "#    plt.figure(figsize=(10,6))\n",
        "#    # Wide-field\n",
        "#    plt.subplot(1,2,1)\n",
        "#    plt.axis('off')\n",
        "#    img_Source = plt.imread(os.path.join(Data_folder[y], file))\n",
        "#    plt.imshow(img_Source, cmap='gray')\n",
        "#    plt.title('Source image',fontsize=15)\n",
        "#    # Prediction\n",
        "#    plt.subplot(1,2,2)\n",
        "#    plt.axis('off')\n",
        "#    img_Prediction = plt.imread(os.path.join(Results_folder[y], prediction_prefix+file))\n",
        "#    plt.imshow(img_Prediction, cmap='gray')\n",
        "#    plt.title('Prediction',fontsize=15)\n",
        "\n",
        "#  interact(show_prediction_mask);"
      ],
      "metadata": {
        "id": "_whs283UxD2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjXv94I1RZxM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 5) Rename and organize predictions\n",
        "os.chdir('/content/filoSkeleton cell predictions')\n",
        "names=sorted(glob.glob('*'+actin_channel+'.tif'))\n",
        "actin_channels,filo_channels,merged_names=[],[],[]\n",
        "for i in names:\n",
        "  temp=i.replace('Predicted_','')\n",
        "  actin_channels.append(temp)\n",
        "  os.rename(i,temp)\n",
        "  merged_names.append(temp.replace(actin_channel+'.tif','RGB.tif'))\n",
        "\n",
        "os.chdir('/content/filoSkeleton filopodia tip predictions')\n",
        "names=sorted(glob.glob('Predicted_*'))\n",
        "for i in names:\n",
        "  temp=i.replace('Predicted_','')\n",
        "  os.rename(i,temp)\n",
        "\n",
        "names=sorted(glob.glob('*'+filo_channel+'.tif'))\n",
        "for i in names:\n",
        "  filo_channels.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6) Use the prediction masks to quantify filopodia\n",
        "print('--Analyzing images--')\n",
        "for q in range(0,len(actin_channels)):\n",
        "    print('--Analyzing '+str(q+1)+' / '+str(len(actin_channels))+' images--')\n",
        "    redValue = 10\n",
        "    blueValue = 1\n",
        "    greenValue = 1\n",
        "    os.chdir('/content/filoSkeleton cell source')\n",
        "    img=cv2.imread(actin_channels[q],-1)\n",
        "    os.chdir('/content/filoSkeleton merged')\n",
        "    img_show=cv2.imread(merged_names[q])\n",
        "    img_show=cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB)\n",
        "    cols, rows = img_show.shape[0],img_show.shape[1]\n",
        "    brightness = np.sum(img_show) / (255 * cols * rows)\n",
        "    minimum_brightness = 0.10\n",
        "    ratio = brightness / minimum_brightness\n",
        "    img_show= cv2.convertScaleAbs(img_show, alpha = 1 / ratio, beta = 0)\n",
        "\n",
        "    img_show1=cv2.imread(merged_names[q])\n",
        "    img_show1=cv2.cvtColor(img_show1, cv2.COLOR_BGR2RGB)\n",
        "    cols, rows = img_show1.shape[0],img_show1.shape[1]\n",
        "    brightness = np.sum(img_show1) / (255 * cols * rows)\n",
        "    minimum_brightness = 0.50\n",
        "    ratio = brightness / minimum_brightness\n",
        "    img_show1= cv2.convertScaleAbs(img_show1, alpha = 1 / ratio, beta = 0)\n",
        "\n",
        "    os.chdir('/content/filoSkeleton cell source')\n",
        "    img_copy=cv2.imread(actin_channels[q])\n",
        "    img_copy[np.where(img_copy>0)]=0\n",
        "    img_copy2=np.copy(img_copy)\n",
        "    img_copy3=np.copy(img_copy)\n",
        "    img_copy4=np.copy(img_copy)\n",
        "    img_copy5=np.copy(img_copy)\n",
        "    img_copy6=np.copy(img_copy)\n",
        "    edgeContours = np.copy(img_copy)\n",
        "    stalk_img1=cv2.imread(actin_channels[q])\n",
        "    filo_img1=cv2.imread(actin_channels[q])\n",
        "    filo_img1[np.where(filo_img1>0)]=0\n",
        "    filo_img2=np.copy(filo_img1)\n",
        "    #red_img=cv2.imread(filo_channels[q],-1)\n",
        "\n",
        "    #read in masks\n",
        "    os.chdir('/content/filoSkeleton cell predictions')\n",
        "    body_stalk=cv2.imread(actin_channels[q],-1)\n",
        "    body,stalk=np.copy(body_stalk),np.copy(body_stalk)\n",
        "    body[np.where(body!=1)]=0\n",
        "    stalk[np.where(body_stalk!=2)]=0\n",
        "    thin_stalk=np.copy(stalk)\n",
        "    kernel = np.ones((7,7), np.uint8)\n",
        "    stalk=cv2.dilate(stalk,kernel)\n",
        "    os.chdir('/content/filoSkeleton filopodia tip predictions')\n",
        "    filo=cv2.imread(filo_channels[q],-1)\n",
        "    os.chdir('/content/filoSkeleton filopodia tip source')\n",
        "    filo_int=cv2.imread(filo_channels[q],-1)\n",
        "\n",
        "    #get cell contours\n",
        "    contours, hierarchy = cv2.findContours(body, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    centroids,areas,perimeters,aspect_ratios,circularity,body_means,edgeCoordinates,cellColors=[],[],[],[],[],[],[],[]\n",
        "    for i in range(0,len(contours)):\n",
        "        M=cv2.moments(contours[i])\n",
        "        if M['m00']!=0:\n",
        "            area=cv2.contourArea(contours[i])/(pixel_micron**2)\n",
        "            if area>25:\n",
        "                centroid=(int(M['m10']/M['m00']),int(M['m01']/M['m00']))\n",
        "                centroids.append(centroid)\n",
        "                area=cv2.contourArea(contours[i])/(pixel_micron**2)\n",
        "                areas.append(area)\n",
        "                img_copy5=cv2.drawContours(img_copy5,contours,i,(147,147,147),-1)\n",
        "                cv2.drawContours(img_copy5,contours,i,(0,0,0,),5)\n",
        "                body_val=filo_int[np.where((img_copy5==list((147,147,147))).all(axis=2))]\n",
        "                body_vals=[]\n",
        "                for val in body_val:\n",
        "                  vals=[str(val)]\n",
        "                  if len(vals)==1:\n",
        "                    body_vals.append(val)\n",
        "                  else:\n",
        "                    body_vals.append(val[2])\n",
        "                body_means.append(np.mean(body_vals))\n",
        "                perimeter=cv2.arcLength(contours[i],True)/(pixel_micron)\n",
        "                perimeters.append(perimeter)\n",
        "                rect=cv2.minAreaRect(contours[i])\n",
        "                wh=rect[1]\n",
        "                w=np.min(wh)\n",
        "                h=np.max(wh)\n",
        "                aspect_ratios.append(float(w)/h)\n",
        "                circularity.append((4*math.pi*area)/(perimeter**2))\n",
        "                img_copy2=cv2.drawContours(np.copy(img_copy),contours,i,white,-1)\n",
        "                img_show=cv2.drawContours(img_show,contours,i,aqua,-1)\n",
        "                img_copy2=cv2.drawContours(img_copy2,contours,i,white,3)\n",
        "                img_copy3=cv2.drawContours(img_copy3,contours,i,aqua,1)\n",
        "                edgeContours = cv2.drawContours(edgeContours,contours,i,(redValue,greenValue,blueValue),1)\n",
        "                edgeCoordinates.append(np.where((edgeContours==list((redValue,greenValue,blueValue))).all(axis=2)))\n",
        "                cellColors.append((redValue,greenValue,blueValue))\n",
        "                redValue = redValue +1\n",
        "                greenValue = greenValue +1\n",
        "                blueValue = blueValue +1\n",
        "\n",
        "    #get all filo contours\n",
        "    filo[np.where((img_copy2==list(white)).all(axis=2))]=0\n",
        "    all_filo=np.copy(filo)\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(all_filo, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    filo_centroids,filo1_tip_body_ratio,filo1_nums,filo1_len,filo1_cent,filo1_coord,filo1_cell,filo1_val,filo1_cell_intensity=[],[],[],[],[],[],[],[],[]\n",
        "    c=1\n",
        "\n",
        "    filo1_i_num=[]\n",
        "\n",
        "    for i in range(0,len(contours)):\n",
        "        M=cv2.moments(contours[i])\n",
        "        if M['m00']!=0:\n",
        "          filo_centroid=(int(M['m10']/M['m00']),int(M['m01']/M['m00']))\n",
        "          filo_centroids.append(centroid)\n",
        "          filo_len=0\n",
        "          filo_img1=np.copy(filo_img2)\n",
        "          cv2.drawContours(filo_img1,contours,i,red,-1)\n",
        "          pts = np.where((filo_img1==list(red)).all(axis=2))\n",
        "          #insert tip intensity here\n",
        "          tip_intensity = np.mean(filo_int[pts])\n",
        "          tip_intensity = np.round(tip_intensity,2)\n",
        "          intensity1=(filo[pts[0],pts[1]])\n",
        "          intensity2=(stalk[pts[0],pts[1]])\n",
        "\n",
        "          #begin stalk tracing\n",
        "          if 1 in intensity1 and 2 in intensity2:\n",
        "              cv2.drawContours(img_show,contours,i,red,-1)\n",
        "              #cv2.drawContours(img_copy7,contours,i,red,-1)\n",
        "              #while body not detected\n",
        "\n",
        "              body_detected = False\n",
        "              stalk_detection_failure = False\n",
        "              stalk_detection_trial = 0\n",
        "              detection_trial_num = 0\n",
        "              prev_centroid = 0\n",
        "              test=[]\n",
        "              cellColor=False\n",
        "\n",
        "              img_copy9=np.copy(img_copy6)\n",
        "              stalk_detection_centroids = []\n",
        "\n",
        "              while body_detected == False:\n",
        "                detection_trial_num = detection_trial_num + 1\n",
        "                if detection_trial_num > 30:\n",
        "                    cv2.drawContours(img_show,contours,i,blue,-1)\n",
        "                    #print(i)\n",
        "                    break\n",
        "                x1,y1=filo_centroid[0],filo_centroid[1]\n",
        "                body_pts=np.where((img_copy3==list(aqua)).all(axis=2))\n",
        "                x2,y2=body_pts[1],body_pts[0]\n",
        "                body_x2, body_y2 = x2, y2\n",
        "                dist=[]\n",
        "                for j in range(len(x2)):\n",
        "                  dist.append(get_distance(x2[j],x1,y2[j],y1))\n",
        "                shortest_len=dist[np.argmin(dist)]/pixel_micron\n",
        "                shortest_cent=x2[np.argmin(dist)],y2[np.argmin(dist)]\n",
        "                x1,y1=shortest_cent\n",
        "                dist=[]\n",
        "                for j in range(len(centroids)):\n",
        "                  x2,y2=centroids[j]\n",
        "                  dist.append(get_distance(x2,x1,y2,y1))\n",
        "             ##   closest_cell=np.argmin(dist)\n",
        "                img_copy7 = np.copy(img_copy6)\n",
        "                img_copy8 = np.copy(all_filo)\n",
        "                img_copy8[np.where(img_copy8 != 0)] = 0\n",
        "                #draw a circle around tip\n",
        "                cv2.circle(img_copy7,filo_centroid,8,pink,2)\n",
        "                if stalk_detection_failure == True:\n",
        "                    stalk_detection_trial = stalk_detection_trial + 1\n",
        "                    stalk_detection_failure_radius = 8 + stalk_detection_trial\n",
        "                    if stalk_detection_failure_radius <= 13:\n",
        "                        cv2.circle(img_copy7,filo_centroid,stalk_detection_failure_radius,pink,2)\n",
        "                tip_pts = np.where((img_copy7==list(pink)).all(axis=2))\n",
        "                #body_intensity = (body[tip_pts[0],tip_pts[1]])\n",
        "                body_intensity = (edgeContours[tip_pts[0],tip_pts[1]])\n",
        "                for colors in cellColors:\n",
        "                    if colors in body_intensity:\n",
        "                        cellColor=colors\n",
        "                        #break\n",
        "\n",
        "\n",
        "                stalk_intensity = (thin_stalk[tip_pts[0],tip_pts[1]])\n",
        "                #if neither body nor stalk are detected - increase radius\n",
        "                intensity_check_counter=1\n",
        "                radius_pixel_input=8\n",
        "\n",
        "                #if body nor stalk is detected - expand radius to find the stalk\n",
        "                while 1 not in body_intensity and len(np.where(stalk_intensity==2)[0]) < 6:\n",
        "                    if intensity_check_counter > 30:\n",
        "                        break\n",
        "                    radius_pixel_input= radius_pixel_input+1\n",
        "                    cv2.circle(img_copy7,filo_centroid,radius_pixel_input,pink,2)\n",
        "                    tip_pts = np.where((img_copy7==list(pink)).all(axis=2))\n",
        "                    body_intensity = (body[tip_pts[0],tip_pts[1]])\n",
        "                    stalk_intensity = (thin_stalk[tip_pts[0],tip_pts[1]])\n",
        "                    intensity_check_counter=intensity_check_counter+1\n",
        "                #if body is detected nearby\n",
        "\n",
        "\n",
        "                if cellColor != False:\n",
        "                    cv2.line(img_copy9,filo_centroid,shortest_cent,yellow,3)\n",
        "     #               filo1_len.append(shortest_len)\n",
        "     #               filo1_cent.append(shortest_cent)\n",
        "\n",
        "                    closest_cell = cellColors.index(cellColor)\n",
        "                    filo1_cell.append(closest_cell+1)\n",
        "                    filo1_cell_intensity.append(body_means[closest_cell])\n",
        "                    reverse_coord=(filo_centroid[1],filo_centroid[0])\n",
        "                    if type(filo_int[reverse_coord])=='list':\n",
        "                      #filo1_val.append(str(filo_int[reverse_coord][2]))\n",
        "                      #filo1_tip_body_ratio.append(str(filo_int[reverse_coord][2]/body_means[closest_cell]))\n",
        "                      filo1_val.append(tip_intensity)\n",
        "                      filo1_tip_body_ratio.append(tip_intensity/body_means[closest_cell])\n",
        "                    else:\n",
        "                      #filo1_val.append(str(filo_int[reverse_coord]))\n",
        "                      #filo1_tip_body_ratio.append(str(filo_int[reverse_coord]/body_means[closest_cell]))\n",
        "                      filo1_val.append(tip_intensity)\n",
        "                      filo1_tip_body_ratio.append(tip_intensity/body_means[closest_cell])\n",
        "                    filo1_coord.append(filo_centroid)\n",
        "                    temp_length = get_distance(shortest_cent[0], filo_centroid[0], shortest_cent[1], filo_centroid[1])\n",
        "                    filo_len = filo_len + temp_length\n",
        "                    cv2.drawContours(filo_img1,contours,i,gray3,-1)\n",
        "                    c=c+1\n",
        "                    body_detected = True\n",
        "                #if body isn't detected, but stalk is\n",
        "                elif 2 in stalk_intensity:\n",
        "                    #get x y coordinates of the pixels that intersect pink tip radius and a thin stalk\n",
        "                    tip_stalk_intersection = np.where((img_copy7==list(pink)).all(axis=2) & (thin_stalk==2))\n",
        "                    img_copy8[tip_stalk_intersection] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    if detection_trial_num > 1 and prev_centroid != 0:\n",
        "                        cv2.circle(img_copy8,prev_centroid,2,0,2)\n",
        "\n",
        "\n",
        "\n",
        "                    #draw contours\n",
        "                    #if 1 contour then move filo_centroid there\n",
        "                    #if >1 contour pick the contour closest to the closest cortex centroid calculated earlier\n",
        "                    tipStalkContours, hierarchy = cv2.findContours(img_copy8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "                    tipStalkCentroids, tipStalkDist = [],[]\n",
        "                    M1s=[]\n",
        "                    for cont in range(len(tipStalkContours)):\n",
        "                        M1=cv2.moments(tipStalkContours[cont])\n",
        "                        M1s.append(M1['m00'])\n",
        "                        if M1['m00']!=0:\n",
        "                          tempCentroid = (int(M1['m10']/M1['m00']),int(M1['m01']/M1['m00']))\n",
        "                          tipStalkCentroids.append(tempCentroid)\n",
        "                          tipStalkDist.append(get_distance(x2=shortest_cent[0],x1=tempCentroid[0],y2=shortest_cent[1],y1=tempCentroid[1]))\n",
        "  #                  print(i)\n",
        "  #                  print(tipStalkCentroids)\n",
        "  #                  print(tipStalkDist)\n",
        "                    M1s= [value for value in M1s if value != 0]\n",
        "                    if len(M1s) > 0:\n",
        "                        tipStalkCentroid=tipStalkCentroids[tipStalkDist.index(min(tipStalkDist))]\n",
        "                        #get distance between tipStalkCentroid and filocentroid\n",
        "                        temp_length= get_distance(tipStalkCentroid[0], filo_centroid[0], tipStalkCentroid[1], filo_centroid[1])\n",
        "                        filo_len = filo_len + temp_length\n",
        "                        cv2.line(img_copy9,filo_centroid,tipStalkCentroid,yellow,3)\n",
        "                        stalk_detection_failure = False\n",
        "                        prev_centroid = filo_centroid\n",
        "                        filo_centroid = tipStalkCentroid\n",
        "                    else:\n",
        "                        stalk_detection_failure = True\n",
        "\n",
        "                else:\n",
        "                    #print('noStalkDetected')\n",
        "                    break\n",
        "                #get yellow line pts\n",
        "                yellow_pts = np.where((img_copy9==list(yellow)).all(axis=2))\n",
        "                #draw yellow line on img_show\n",
        "                img_show[yellow_pts] = yellow\n",
        "\n",
        "              if body_detected == True:\n",
        "                  filo1_len.append(filo_len/pixel_micron)\n",
        "                  filo1_i_num.append(i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #                if len(tipStalkContours) == 1:\n",
        "    #                    M1=cv2.moments(tipStalkContours[0])\n",
        "    #                    if M1['m00']!=0:\n",
        "    #                      tipStalkCentroid=(int(M1['m10']/M1['m00']),int(M1['m01']/M1['m00']))\n",
        "    #                      cv2.line(img_copy9,filo_centroid,tipStalkCentroid,yellow,3)\n",
        "    #                      filo_centroid = tipStalkCentroid\n",
        "    #                elif len(tipStalkContours) >1:\n",
        "    #                    tipStalkCentroids=[]\n",
        "    #                    tipStalkDist=[]\n",
        "    #                    for cont in range(0,len(tipStalkContours)):\n",
        "    #                        M1=cv2.moments(tipStalkContours[cont])\n",
        "    #                        if M1['m00']!=0:\n",
        "    #                          tipStalkCentroids.append((int(M1['m10']/M1['m00']),int(M1['m01']/M1['m00'])))\n",
        "    #                          tipStalkDist.append(get_distance(x2=shortest_cent[0],x1=filo_centroid[0],y2=shortest_cent[1],y1=filo_centroid[1]))\n",
        "    #                          tipStalkCentroidn=tipStalkCentroids[tipStalkDist.index(min(tipStalkDist))]\n",
        "    #                          cv2.line(img_copy9,filo_centroid,tipStalkCentroid,yellow,3)\n",
        "    #                          filo_centroid = tipStalkCentroid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #                          tipStalkCentroid=(int(M1['m10']/M1['m00']),int(M1['m01']/M1['m00']))\n",
        "    #                          cv2.line(img_copy9,filo_centroid,tipStalkCentroid,yellow,3)\n",
        "    #                          filo_centroid = tipStalkCentroid\n",
        "\n",
        "\n",
        "           #     plt.imshow(img_copy8)\n",
        "           #     plt.imshow(img_copy7)\n",
        "\n",
        "\n",
        "\n",
        "            #elif 1 stalk position detected\n",
        "            #draw line to new stalk position and repeat\n",
        "            #elif >1 stalk positions detected\n",
        "            #remove the previous position\n",
        "            #find the distance from both stalk positions - find the closest to the body - move to that new position and repeat\n",
        "            #insurance - if this process goes over x loops - mark tip as blue and end\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #summarize filo information\n",
        "    rootName = actin_channels[q].replace(actin_channel+'.tif','')\n",
        "    filo1_protein=np.repeat(filo_name,len(filo1_len))\n",
        "    filo_names=np.repeat(rootName,len(filo1_len))\n",
        "    avg_len=np.mean(filo1_len)\n",
        "\n",
        "    #filo_output dataframe\n",
        "    filo1_protein=np.repeat(filo_name,len(filo1_len))\n",
        "    filo_names=np.repeat(rootName,len(filo1_len))\n",
        "\n",
        "\n",
        "    filo1_={'Experiment Name':filo_names,'Cell Assignment':filo1_cell,'Cell Body Average Intensity':filo1_cell_intensity,'Length (um)':filo1_len,'Filo Tip Centroid Coordinates':filo1_coord,'Filo Tip Intensity':filo1_val,'Filo Tip/Cell Body Ratio':filo1_tip_body_ratio,'Filopodia Proteins Present':filo1_protein}\n",
        "    filo_output=pd.DataFrame(filo1_)\n",
        "\n",
        "    #summarize and prepare for result outputs\n",
        "    filo1_cell_ind=np.asarray(filo1_cell)\n",
        "    final1_2,final1,final2,finalprojection=[],[],[],[]\n",
        "    for i in range(1,len(centroids)+1):\n",
        "      final1.append(len(np.where(filo1_cell_ind==i)[0]))\n",
        "    cell_num=list(range(1,len(centroids)+1))\n",
        "\n",
        "    filos1_2_micron=[i/j for i,j in zip(final1_2,perimeters)]\n",
        "    filos1_micron=[i/j for i,j in zip(final1,perimeters)]\n",
        "    filos2_micron=[i/j for i,j in zip(final2,perimeters)]\n",
        "    projection_micron=[i/j for i,j in zip(finalprojection,perimeters)]\n",
        "\n",
        "    #combine lists into a pandas df and export\n",
        "    os.chdir('/content')\n",
        "    if not os.path.exists('filoSkeleton output'):\n",
        "        os.makedirs('filoSkeleton output')\n",
        "    exp_num_cell=np.repeat(rootName,len(centroids))\n",
        "    cell_num=[]\n",
        "    for i in range(0,len(centroids)):\n",
        "      cell_num.append(i+1)\n",
        "\n",
        "    #calculate filopodia normalized to perimeter\n",
        "    filos_perimeter = [i/j for i,j in zip(final1,perimeters)]\n",
        "\n",
        "    #finish combining lists\n",
        "    cell_dict={'Experiment Name':exp_num_cell,'Cell Number':cell_num,'Aspect Ratio':aspect_ratios,'Circularity':circularity,'Average Body Intensity':body_means,'Filos/Cell ('+filo_name+')':final1,'Filos/Perimeter (Filos/micron)':filos_perimeter, 'Avg Filo Length (um)':avg_len,'Cell Area (um^2)':areas,'Perimeter (um)':perimeters}\n",
        "    cell_output=pd.DataFrame(cell_dict)\n",
        "    cell_output.to_csv('filoSkeleton output/'+str(rootName.replace(actin_channel+'.tif',''))+'_Cell_Output_.csv',index=False)\n",
        "    filo_output.to_csv('filoSkeleton output/'+str(rootName.replace(actin_channel+'.tif',''))+'_Filo_Output_.csv',index=False)\n",
        "\n",
        "    #make annotation\n",
        "    fig, final=plt.subplots(1,2)\n",
        "    final[0].imshow(img_show1)\n",
        "    final[0].set_title('Merged')\n",
        "    final[0].axis('off')\n",
        "    final[1].imshow(img_show)\n",
        "    final[1].set_title('filoSkeleton')\n",
        "    final[1].axis('off')\n",
        "    plt.savefig('filoSkeleton output/'+str(rootName.replace(actin_channel+'.tif',''))+'_Annotation.tiff',dpi=Annotation_DPI)\n",
        "\n"
      ],
      "metadata": {
        "id": "hGtVGuBh1LAn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7) Pool together the analysis\n",
        "os.chdir('/content')\n",
        "cell_outputs=sorted(glob.glob(\"filoSkeleton output/*Cell_Output_.csv\"))\n",
        "filo_outputs=sorted(glob.glob('filoSkeleton output/*Filo_Output_.csv'))\n",
        "cell_dfs = (pd.read_csv(f) for f in cell_outputs)\n",
        "cell_dfs = pd.concat(cell_dfs, ignore_index=True)\n",
        "filo_dfs = (pd.read_csv(f) for f in filo_outputs)\n",
        "filo_dfs = pd.concat(filo_dfs, ignore_index=True)\n",
        "cell_dfs.to_csv('filoSkeleton output/Total_Cell_Output.csv',index=False)\n",
        "filo_dfs.to_csv('filoSkeleton output/Total_Filo_Output.csv',index=False)"
      ],
      "metadata": {
        "id": "9k-S6EC6vprV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8) Optional data visualization\n",
        "os.chdir('/content')\n",
        "cell_file=pd.read_csv('filoSkeleton output/Total_Cell_Output.csv')\n",
        "if not os.path.exists('filoSkeleton output/Plots'):\n",
        "        os.makedirs('filoSkeleton output/Plots')\n",
        "\n",
        "if comparative_analysis ==True:\n",
        "  prompt1=[Condition_1,Condition_2,Condition_3,Condition_4]\n",
        "  if '' in prompt1:\n",
        "    prompt1.remove('')\n",
        "    if '' in prompt1:\n",
        "      prompt1.remove('')\n",
        "  prompt1=str(len(prompt1))\n",
        "  parameter=['Aspect Ratio','Circularity','Filos/Cell ('+filo_name+')','Filos/Perimeter (Filos/micron)','Avg Filo Length (um)','Cell Area (um^2)','Perimeter (um)']\n",
        "\n",
        "  data= cell_file\n",
        "  for t in range(0,len(parameter)):\n",
        "    if prompt1=='2':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "\n",
        "      print('')\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('--Mann-Whitney U Test--')\n",
        "      kruskal=stats.mannwhitneyu(set1[parameter[t]],set2[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #t-test\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('--T-test--')\n",
        "      t_test=rp.ttest(group1=set1[parameter[t]],group1_name=Condition_1,\n",
        "              group2=set2[parameter[t]],group2_name=Condition_2)\n",
        "      print(t_test)\n",
        "      means=np.round([t_test[0]['Mean'][0],t_test[0]['Mean'][1]],decimals=2)\n",
        "      stds=np.round([t_test[0]['SD'][0],t_test[0]['SD'][1]],decimals=2)\n",
        "      bars=t_test[0]['Variable'][0],t_test[0]['Variable'][1]\n",
        "      x_pos=x_pos=list(np.arange(len(bars)))\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      temp=temp.replace('/','_')\n",
        "      fig.savefig('/filoSkeleton output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')\n",
        "\n",
        "    if prompt1=='3':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          ind3=data.loc[data['Experiment Name'].str.contains(Condition_3)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "          data['Condition'][ind3]=Condition_3\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "      set3=data.loc[data['Condition'].str.contains(Condition_3)].reset_index(drop=True)\n",
        "\n",
        "      #kruskal-wallis\n",
        "      print('')\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('-----Kruskal-Wallis Test-----')\n",
        "      kruskal=stats.kruskal(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #ANOVA\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('-----ANOVA-----')\n",
        "      an=stats.f_oneway(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]])\n",
        "      vals=data[parameter[t]].tolist()\n",
        "      names=data['Condition'].tolist()\n",
        "      tukey=pairwise_tukeyhsd(endog=vals,\n",
        "                              groups=names,\n",
        "                              alpha=0.05)\n",
        "      print(tukey)\n",
        "      means=np.round([np.mean(set1[parameter[t]]),np.mean(set2[parameter[t]]),np.mean(set3[parameter[t]])],decimals=2)\n",
        "      stds=np.round([np.std(set1[parameter[t]]),np.std(set2[parameter[t]]),np.std(set3[parameter[t]])],decimals=2)\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]]),len(set3[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5],palp[1]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2,Condition_3], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      temp=temp.replace('/','_')\n",
        "      fig.savefig('/filoSkeleton output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')\n",
        "\n",
        "\n",
        "    if prompt1=='4':\n",
        "      #setup\n",
        "      #data=data.dropna()\n",
        "      if 'Condition' not in data.columns:\n",
        "          ind1=data.loc[data['Experiment Name'].str.contains(Condition_1)].index\n",
        "          ind2=data.loc[data['Experiment Name'].str.contains(Condition_2)].index\n",
        "          ind3=data.loc[data['Experiment Name'].str.contains(Condition_3)].index\n",
        "          ind4=data.loc[data['Experiment Name'].str.contains(Condition_4)].index\n",
        "          data.insert(0,'Condition',0)\n",
        "          data['Condition'][ind1]=Condition_1\n",
        "          data['Condition'][ind2]=Condition_2\n",
        "          data['Condition'][ind3]=Condition_3\n",
        "          data['Condition'][ind4]=Condition_4\n",
        "      set1=data.loc[data['Condition'].str.contains(Condition_1)].reset_index(drop=True)\n",
        "      set2=data.loc[data['Condition'].str.contains(Condition_2)].reset_index(drop=True)\n",
        "      set3=data.loc[data['Condition'].str.contains(Condition_3)].reset_index(drop=True)\n",
        "      set4=data.loc[data['Condition'].str.contains(Condition_4)].reset_index(drop=True)\n",
        "\n",
        "      #kruskal-wallis\n",
        "      print('')\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('-----Kruskal-Wallis Test-----')\n",
        "      kruskal=stats.kruskal(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]],set4[parameter[t]])\n",
        "      print(kruskal)\n",
        "      print(' ')\n",
        "\n",
        "      #ANOVA\n",
        "      print('--'+parameter[t]+'--')\n",
        "      print('-----ANOVA-----')\n",
        "      an=stats.f_oneway(set1[parameter[t]],set2[parameter[t]],set3[parameter[t]],set4[parameter[t]])\n",
        "      vals=data[parameter[t]].tolist()\n",
        "      names=data['Condition'].tolist()\n",
        "      tukey=pairwise_tukeyhsd(endog=vals,\n",
        "                              groups=names,\n",
        "                              alpha=0.05)\n",
        "      print(tukey)\n",
        "      means=np.round([np.mean(set1[parameter[t]]),np.mean(set2[parameter[t]]),np.mean(set3[parameter[t]]),np.mean(set4[parameter[t]])],decimals=2)\n",
        "      stds=np.round([np.std(set1[parameter[t]]),np.std(set2[parameter[t]]),np.std(set3[parameter[t]]),np.std(set4[parameter[t]])],decimals=2)\n",
        "      means_stds=[]\n",
        "      for i in range(0,len(means)):\n",
        "          means_stds.append(str(means[i])+'±'+str(stds[i]))\n",
        "      nums=[len(set1[parameter[t]]),len(set2[parameter[t]]),len(set3[parameter[t]]),len(set4[parameter[t]])]\n",
        "\n",
        "      #violin plot\n",
        "      pal=sns.color_palette()\n",
        "      palp=sns.color_palette(\"husl\",8)\n",
        "      cols=[palp[3],palp[5],palp[1],palp[7]]\n",
        "      dpi=150\n",
        "      fig,ax1=plt.subplots()\n",
        "      sns.violinplot(data = data[['Condition',parameter[t]]], x=parameter[t], y=\"Condition\", order=[Condition_1,Condition_2,Condition_3,Condition_4], palette=cols, showmeans=True,inner=None)\n",
        "      ax1.set_xlabel(parameter[t])\n",
        "      for i in range(len(means_stds)):\n",
        "          ax1.annotate(str(means_stds[i]+'\\nn='+str(nums[i])),xy=(means[i],i),horizontalalignment='center',verticalalignment='center')\n",
        "      ax1.xaxis.set_label_position('top')\n",
        "      ax1.xaxis.tick_top()\n",
        "      temp=parameter[t].split('(')[0]\n",
        "      temp=temp.replace('/','_')\n",
        "      fig.savefig('/filoSkeleton output/Plots/'+temp+ '.tiff',dpi=dpi,bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "UHC_d8EMi60t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9) Download filoSkeleton_Output locally\n",
        "os.chdir('/content/filoSkeleton output')\n",
        "if not os.path.exists('Annotations'):\n",
        "        os.makedirs('Annotations')\n",
        "if not os.path.exists('Individual Experiments'):\n",
        "        os.makedirs('Individual Experiments')\n",
        "\n",
        "annot_files=glob.glob('*.tiff')\n",
        "ind_files=glob.glob('*_.csv')\n",
        "\n",
        "for i in ind_files:\n",
        "  os.replace(i,'Individual Experiments/'+i)\n",
        "for i in annot_files:\n",
        "  os.replace(i,'Annotations/'+i)\n",
        "\n",
        "os.chdir('/content')\n",
        "shutil.make_archive(output_filename, 'zip', 'filoSkeleton output')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_filename+'.zip')"
      ],
      "metadata": {
        "id": "JjbGxHwri5Rc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}